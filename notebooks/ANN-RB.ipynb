{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import snntorch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim.adam import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    \"\"\"\n",
    "        A standard in_dim-64-64-out_dim Feed Forward Neural Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "            Initialize the network and set up the layers.\n",
    "\n",
    "            Parameters:\n",
    "                in_dim - input dimensions as an int\n",
    "                out_dim - output dimensions as an int\n",
    "\n",
    "                Return:\n",
    "                None\n",
    "        \"\"\"\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "            Runs a forward pass on the neural network.\n",
    "\n",
    "            Parameters:\n",
    "                obs - observation to pass as input\n",
    "\n",
    "            Return:\n",
    "                output - the output of our forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "\n",
    "        activation1 = torch.relu(self.layer1(obs))\n",
    "        activation2 = torch.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 5\n",
    "act_dim = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_batch = 4800  # Number of timesteps to run per batch\n",
    "max_timesteps_per_episode = 1600  # Max number of timesteps per episode\n",
    "n_updates_per_iteration = 5  # Number of times to update actor/critic per iteration\n",
    "lr = 0.005  # Learning rate of actor optimizer\n",
    "gamma = 0.95  # Discount factor to be applied when calculating Rewards-To-Go\n",
    "clip = 0.2  # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
    "\n",
    "# Miscellaneous parameters\n",
    "render = True  # If we should render during rollout\n",
    "render_every_i = 10  # Only render every n iterations\n",
    "save_freq = 10  # How often we save in number of iterations\n",
    "seed = None  # Sets the seed of our program, used for reproducibility of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "#### actor and critic networks\n",
    "#### optimizers\n",
    "#### covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance for the actor network\n",
    "actor = FeedForwardNN(obs_dim, act_dim)  # ALG STEP 1\n",
    "\n",
    "# create an instance for the critic network\n",
    "critic = FeedForwardNN(obs_dim, 1)\n",
    "\n",
    "# Initialize optimizers for actor and critic\n",
    "actor_optim = Adam(actor.parameters(), lr=lr)\n",
    "critic_optim = Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "cov_var = torch.full(size=(act_dim,), fill_value=0.5)\n",
    "cov_mat = torch.diag(cov_var)\n",
    "\n",
    "replay_buffer = ReplayBuffer(max_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "#### rewards to go calculation\n",
    "#### evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rtgs(batch_rews):\n",
    "    \"\"\"\n",
    "        Compute the Reward-To-Go of each timestep in a batch given the rewards.\n",
    "\n",
    "        Parameters:\n",
    "            batch_rews - the rewards in a batch, Shape: (number of episodes, number of timesteps per episode)\n",
    "\n",
    "        Return:\n",
    "            batch_rtgs - the rewards to go, Shape: (number of timesteps in batch)\n",
    "    \"\"\"\n",
    "    # The rewards-to-go (rtg) per episode per batch to return.\n",
    "    # The shape will be (num timesteps per episode)\n",
    "    batch_rtgs = []\n",
    "\n",
    "    # Iterate through each episode\n",
    "    for ep_rews in reversed(batch_rews):\n",
    "\n",
    "        discounted_reward = 0  # The discounted reward so far\n",
    "\n",
    "        # Iterate through all rewards in the episode. We go backwards for smoother calculation of each\n",
    "        # discounted return (think about why it would be harder starting from the beginning)\n",
    "        for rew in reversed(ep_rews):\n",
    "            discounted_reward = rew + discounted_reward * gamma\n",
    "            batch_rtgs.insert(0, discounted_reward)\n",
    "\n",
    "    # Convert the rewards-to-go into a tensor\n",
    "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "\n",
    "    return batch_rtgs\n",
    "\n",
    "def evaluate(batch_obs, batch_acts):\n",
    "    \"\"\"\n",
    "        Estimate the values of each observation, and the log probs of\n",
    "        each action in the most recent batch with the most recent\n",
    "        iteration of the actor network. Should be called from learn.\n",
    "\n",
    "        Parameters:\n",
    "            batch_obs - the observations from the most recently collected batch as a tensor.\n",
    "                        Shape: (number of timesteps in batch, dimension of observation)\n",
    "            batch_acts - the actions from the most recently collected batch as a tensor.\n",
    "                        Shape: (number of timesteps in batch, dimension of action)\n",
    "\n",
    "        Return:\n",
    "            V - the predicted values of batch_obs\n",
    "            log_probs - the log probabilities of the actions taken in batch_acts given batch_obs\n",
    "    \"\"\"\n",
    "\n",
    "    # Query critic network for a value V for each batch_obs. Shape of V should be same as batch_rtgs\n",
    "    V = critic(batch_obs).squeeze()\n",
    "\n",
    "    print(batch_obs)\n",
    "\n",
    "    # Calculate the log probabilities of batch actions using most recent actor network.\n",
    "    # This segment of code is similar to that in get_action()\n",
    "    mean = actor(batch_obs)\n",
    "    dist = MultivariateNormal(mean, cov_mat)\n",
    "    log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "    # Return the value vector V of each observation in the batch\n",
    "    # and log probabilities log_probs of each action in the batch\n",
    "    return V, log_probs\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\"\n",
    "        Queries an action from the actor network, should be called from rollout.\n",
    "\n",
    "        Parameters:\n",
    "            obs - the observation at the current timestep\n",
    "\n",
    "        Return:\n",
    "            action - the action to take, as a numpy array\n",
    "            log_prob - the log probability of the selected action in the distribution\n",
    "    \"\"\"\n",
    "    # Query the actor network for a mean action\n",
    "    mean = actor(obs)\n",
    "\n",
    "    # Create a distribution with the mean action and std from the covariance matrix above.\n",
    "    dist = MultivariateNormal(mean, cov_mat)\n",
    "\n",
    "    # Sample an action from the distribution\n",
    "    action = dist.sample()\n",
    "\n",
    "    # Calculate the log probability for that action\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    # Return the sampled action and the log probability of that action in our distribution\n",
    "    return action.detach().numpy(), log_prob.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset, First observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "obs = torch.tensor([300.0000, 450.0000,   0.0000,   4.7124,   0.0000])\n",
    "\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get action and log prob of first observation\n",
    "This is a one iteration of get_action() method\n",
    "\n",
    "- Get network output to be used as a mean for the distribution.\n",
    "\n",
    "- Create a distribution with the mean action and std from the covariance matrix above. <br/>\n",
    "For more information on how this distribution works, check out Andrew Ng's lecture on it: <br/>\n",
    "https://www.youtube.com/watch?v=JjB58InuTqM <br/>\n",
    "\n",
    "- Sample an action from the distribution\n",
    "\n",
    "- Calculate the log probability for that action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of distribution  tensor([  9.5382, -10.3285,   2.1436,  -3.4395,  18.5752])\n",
      "action to take  [  9.466348  -11.078942    1.7510126  -3.4813359  17.64109  ]\n",
      "log probability of the action  tensor(-4.4588)\n"
     ]
    }
   ],
   "source": [
    "mean = actor(obs)\n",
    "\n",
    "dist = MultivariateNormal(mean, cov_mat)\n",
    "\n",
    "action = dist.sample()\n",
    "\n",
    "log_prob = dist.log_prob(action)\n",
    "\n",
    "print(\"mean of distribution \", mean.detach())\n",
    "print(\"action to take \", action.detach().numpy())\n",
    "print(\"log probability of the action \",log_prob.detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.685039  -9.983261   2.6699722 -4.2243786 18.117476 ]\n",
      "tensor(-4.1052)\n"
     ]
    }
   ],
   "source": [
    "act , log = get_action(obs)\n",
    "\n",
    "print(act)\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout\n",
    "\n",
    "- batch observations collected from simulation, first obs O_0 is from reset [0, n-1]\n",
    "- batch actions collected from querying the network given observations [1, n]\n",
    "- batch log probabilities collected from querying the network given observations [1, n]\n",
    "- batch rewards collected from simulation after taking an action. [1, n]\n",
    "- batch lenghts stores batch and episode lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch acts  tensor([[  9.0482, -10.4983,   2.8699,  -2.3475,  18.9719],\n",
      "        [ 10.7451, -10.0399,   1.1352,  -4.5502,  18.8192],\n",
      "        [ 10.7088, -10.7789,   1.3703,  -2.2861,  18.8478],\n",
      "        [ 10.1925,  -9.6547,   2.3445,  -3.8386,  18.1726],\n",
      "        [  9.4776, -10.6591,   0.6093,  -2.4303,  17.7746],\n",
      "        [  8.3343,  -9.7671,   2.0907,  -4.7859,  19.6051]])\n",
      "batch log probs  tensor([-5.0080, -6.4758, -5.7374, -4.1057, -6.5133, -7.3456])\n",
      "batch rtg  tensor([-3.2398, -2.2304, -1.1539, -3.2398, -2.2304, -1.1539])\n"
     ]
    }
   ],
   "source": [
    "# rollout\n",
    "\n",
    "# batch obs\n",
    "batch_obs = torch.tensor([[300.0000, 450.0000,   0.0000,   4.7124,   0.0000],\n",
    "        [299.9333, 450.0000,   3.9801,   4.7124,   0.0000],\n",
    "        [299.8003, 450.0000,   7.9404,   4.7124,   0.0000],\n",
    "        [300.0000, 450.0000,   0.0000,   4.7124,   0.0000],\n",
    "        [299.9333, 450.0000,   3.9801,   4.7124,   0.0000],\n",
    "        [299.8003, 450.0000,   7.9404,   4.7124,   0.0000]])\n",
    "\n",
    "batch_acts = []\n",
    "batch_log_probs = []\n",
    "\n",
    "for i in range(len(batch_obs)):\n",
    "        action, log_prob = get_action(batch_obs[i])\n",
    "        batch_acts.append(action)\n",
    "        batch_log_probs.append(log_prob)\n",
    "\n",
    "batch_rews = [[-1.1209449646284, -1.134160306418572, -1.1539176437616128], [-1.1209449646284, -1.134160306418572, -1.1539176437616128]]\n",
    "\n",
    "batch_rtgs = compute_rtgs(batch_rews)\n",
    "\n",
    "batch_lens = [3, 3]\n",
    "\n",
    "batch_acts = torch.tensor(np.array(batch_acts), dtype=torch.float)\n",
    "batch_log_probs = torch.tensor(np.array(batch_log_probs), dtype=torch.float)\n",
    "\n",
    "\n",
    "for i in range(len(batch_obs)):\n",
    "        replay_buffer.push(batch_obs[i], batch_acts[i], batch_rews[i])\n",
    "\n",
    "\n",
    "\n",
    "print(\"batch acts \",batch_acts)\n",
    "print(\"batch log probs \",batch_log_probs)\n",
    "print(\"batch rtg \",batch_rtgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate one iteration (for demonstration)\n",
    "- Query critic network for a value V for each batch_obs. Shape of V should be same as batch_rtgs\n",
    "- Calculate the log probabilities of batch actions using most recent actor network.\n",
    "This segment of code is similar to that in get_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5])\n",
      "tensor([14.5844, 14.4048, 14.2315, 14.5844, 14.4048, 14.2315])\n",
      "tensor([-5.0080, -6.4758, -5.7373, -4.1057, -6.5133, -7.3456])\n"
     ]
    }
   ],
   "source": [
    "V = critic(batch_obs).squeeze()\n",
    "\n",
    "mean = actor(batch_obs)\n",
    "dist = MultivariateNormal(mean, cov_mat)\n",
    "log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "print(mean.shape)\n",
    "print(V.detach())\n",
    "print(log_probs.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[300.0000, 450.0000,   0.0000,   4.7124,   0.0000],\n",
      "        [299.9333, 450.0000,   3.9801,   4.7124,   0.0000],\n",
      "        [299.8003, 450.0000,   7.9404,   4.7124,   0.0000],\n",
      "        [300.0000, 450.0000,   0.0000,   4.7124,   0.0000],\n",
      "        [299.9333, 450.0000,   3.9801,   4.7124,   0.0000],\n",
      "        [299.8003, 450.0000,   7.9404,   4.7124,   0.0000]])\n",
      "tensor([14.5844, 14.4048, 14.2315, 14.5844, 14.4048, 14.2315],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor([-17.8242, -16.6352, -15.3854, -17.8242, -16.6352, -15.3854])\n"
     ]
    }
   ],
   "source": [
    "# Calculate advantage at k-th iteration\n",
    "V, _ = evaluate(batch_obs, batch_acts)\n",
    "A_k = batch_rtgs - V.detach()\n",
    "\n",
    "print(V)\n",
    "print(A_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1086, -0.0186,  1.1272, -1.1086, -0.0186,  1.1272])\n"
     ]
    }
   ],
   "source": [
    "# normalizing the advantage\n",
    "A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "print(A_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the network through a number of iterations\n",
    "\n",
    "- Calculate V_phi and pi_theta(a_t | s_t)\n",
    "- Calculate the ratio pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t) <br/>\n",
    "NOTE: we just subtract the logs, which is the same as<br/>\n",
    "dividing the values and then canceling the log with e^log.<br/>\n",
    "For why we use log probabilities instead of actual probabilities,<br/>\n",
    "here's a great explanation:<br/>\n",
    "https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms<br/>\n",
    "TL;DR makes gradient ascent easier behind the scenes.<br/>\n",
    "- Calculate surrogate losses.\n",
    "- Calculate actor and critic losses. <br/>\n",
    "NOTE: we take the negative min of the surrogate losses because we're trying to maximize <br/>\n",
    "the performance function, but Adam minimizes the loss. So minimizing the negative <br/>\n",
    "performance function maximizes it. <br/>\n",
    "- Calculate gradients and perform backward propagation for actor and critic network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## update start ##\n",
      "tensor([-5.1444, -6.9009, -5.5648, -3.8300, -7.4699, -7.2108])\n",
      "tensor([-7.6773, -7.7762, -7.8732, -7.6773, -7.7762, -7.8732],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor([-24573.3047, -24026.8809, -24084.8027, -24505.0898, -23910.9277,\n",
      "        -23803.5879], grad_fn=<SubBackward0>)\n",
      "* V \n",
      " tensor([-7.6773, -7.7762, -7.8732, -7.6773, -7.7762, -7.8732])\n",
      "* curr_log_probs \n",
      " tensor([-24573.3047, -24026.8809, -24084.8027, -24505.0898, -23910.9277,\n",
      "        -23803.5879])\n",
      "* ratios \n",
      " tensor([0., 0., 0., 0., 0., 0.])\n",
      "* surr1 \n",
      " tensor([-0., -0., 0., -0., -0., 0.])\n",
      "* surr2 \n",
      " tensor([-0.8862, -0.0163,  0.9025, -0.8862, -0.0163,  0.9025])\n",
      "tensor(0.3008, grad_fn=<MeanBackward0>)\n",
      "tensor(31.8651, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_updates_per_iteration = 1\n",
    "\n",
    "actor_loss_arr = []\n",
    "critic_loss_arr = []\n",
    "\n",
    "# observations, actions, rewards = replay_buffer.sample(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(num_updates_per_iteration):\n",
    "\n",
    "    print(\"## update start ##\")\n",
    "\n",
    "    V, curr_log_probs = evaluate(batch_obs, batch_acts)\n",
    "\n",
    "    ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "    surr1 = ratios * A_k\n",
    "    surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * A_k\n",
    "\n",
    "    actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "    critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "\n",
    "    actor_loss_arr.append(actor_loss)\n",
    "    critic_loss_arr.append(critic_loss)\n",
    "\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward(retain_graph=True)\n",
    "    actor_optim.step()\n",
    "\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n",
    "\n",
    "\n",
    "    print(\"* V \\n\",V.detach())\n",
    "    print(\"* curr_log_probs \\n\",curr_log_probs.detach())\n",
    "    print(\"* ratios \\n\",ratios.detach())\n",
    "    print(\"* surr1 \\n\",surr1.detach())\n",
    "    print(\"* surr2 \\n\",surr2.detach())\n",
    "\n",
    "print(actor_loss)\n",
    "print(critic_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
