{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import snntorch as snn\n",
    "import snntorch.functional as SF\n",
    "import snntorch.spikegen as spikegen\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spike_trains(observation, num_steps, threshold, shift):\n",
    "    \"\"\"\n",
    "    Generate spike trains from a single observation using a fixed global threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - observation: A tensor representing the observation ([observation_dim]).\n",
    "    - num_steps: The number of timesteps for the spike train.\n",
    "    - threshold: A single global threshold value to be used for normalization.\n",
    "    \n",
    "    Returns:\n",
    "    - spike_trains: Tensor of spike trains.\n",
    "    \"\"\"\n",
    "    \n",
    "    shift = shift.numpy()\n",
    "\n",
    "    # Normalize and clip observation\n",
    "    shifted_obs = np.add(observation, shift) \n",
    "\n",
    "    # torch version\n",
    "    #shifted_obs = observation + shift\n",
    "\n",
    "\n",
    "    normalized_obs = shifted_obs / (threshold + 1e-6)  # Avoid division by zero\n",
    "\n",
    "    normalized_obs /= 2\n",
    "    \n",
    "    normalized_obs = normalized_obs.clamp(0, 1)  # Clip values to be within [0, 1]\n",
    "\n",
    "    \n",
    "    # Generate spike trains\n",
    "    spike_trains = spikegen.rate(normalized_obs, num_steps=num_steps)\n",
    "    \n",
    "    # torch version\n",
    "    #return spike_trains\n",
    "\n",
    "    return spike_trains.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spike_trains_batched(observations, num_steps, threshold, shift):\n",
    "    \"\"\"\n",
    "    Generate spike trains from batched observations using a fixed global threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - observations: A tensor representing the batched observations ([batch_size, observation_dim]).\n",
    "    - num_steps: The number of timesteps for the spike train.\n",
    "    - threshold: A single global threshold value to be used for normalization.\n",
    "    - shift: A value to shift the observation range to handle negative values.\n",
    "    \n",
    "    Returns:\n",
    "    - spike_trains: Tensor of spike trains with shape (batch_size, num_steps, observation_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    shift = shift.numpy()\n",
    "\n",
    "\n",
    "    # Normalize and shift observations\n",
    "    normalized_obs = np.add(observations, shift) / (2 * (threshold + 1e-6))  # Avoid division by zero\n",
    "    normalized_obs = normalized_obs.clamp(0, 1)  # Clip values to [0, 1]\n",
    "\n",
    "    # Generate spike trains for each observation in the batch\n",
    "    spike_trains = spikegen.rate(normalized_obs, num_steps=num_steps)\n",
    "    \n",
    "    # Rearrange the output to have shape (batch_size, num_steps, observation_dim)\n",
    "    spike_trains = spike_trains.permute(1, 0, 2)\n",
    "    \n",
    "    # torch version\n",
    "    #return spike_trains\n",
    "\n",
    "    return spike_trains.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spike_counts(spike_trains):\n",
    "    \"\"\"\n",
    "    Get the total number of spikes for each neuron over all timesteps.\n",
    "    \n",
    "    Parameters:\n",
    "    - spike_trains: Tensor of spike trains with shape [num_steps, observation_dim].\n",
    "    \n",
    "    Returns:\n",
    "    - Array of spike counts for each neuron.\n",
    "    \"\"\"\n",
    "    spike_counts = torch.sum(spike_trains, dim=0)\n",
    "    return spike_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spike_counts_batched(spike_trains):\n",
    "    \"\"\"\n",
    "    Get the total number of spikes for each neuron over all timesteps for batched spike trains.\n",
    "    \n",
    "    Parameters:\n",
    "    - spike_trains: Tensor of spike trains with shape [batch_size, num_steps, observation_dim].\n",
    "    \n",
    "    Returns:\n",
    "    - Array of spike counts for each neuron in each observation (shape: [batch_size, observation_dim]).\n",
    "    \"\"\"\n",
    "    # Sum over the time dimension (dim=1) to get spike counts for each neuron in each observation\n",
    "    spike_counts = torch.sum(spike_trains, dim=1)\n",
    "    \n",
    "    return spike_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_first_spike_batched(spike_trains):\n",
    "    \"\"\"\n",
    "    Decodes the first spike time from batched spike trains using the 'time to first spike' method.\n",
    "    \n",
    "    Parameters:\n",
    "        spike_trains - The batched spike trains with shape (batch_size, num_steps, num_neurons).\n",
    "    \n",
    "    Returns:\n",
    "        decoded_vector - A tensor representing the first spike times for each neuron in each batch with gradients retained.\n",
    "    \"\"\"\n",
    "    batch_size, num_steps, num_neurons = spike_trains.shape\n",
    "\n",
    "    # Create a tensor with time steps and retain gradients\n",
    "    time_tensor = torch.arange(1, num_steps + 1, dtype=torch.float32, requires_grad=True).unsqueeze(0).unsqueeze(2).expand(batch_size, num_steps, num_neurons)\n",
    "\n",
    "    # Multiply spike_trains by the time tensor, masking out non-spike entries\n",
    "    spike_times = spike_trains * time_tensor\n",
    "\n",
    "    # Set all zero entries (no spike) to a very high value (greater than num_steps)\n",
    "    spike_times = spike_times + (1 - spike_trains) * (num_steps+1)\n",
    "\n",
    "    # Find the minimum value in each column (i.e., first spike) for each batch\n",
    "    first_spike_times, _ = spike_times.min(dim=1)\n",
    "\n",
    "    # Ensure that this tensor retains gradients\n",
    "    return first_spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_first_spike(spike_trains):\n",
    "    \"\"\"\n",
    "    Decodes the first spike time from spike trains using the 'time to first spike' method.\n",
    "    \n",
    "    Parameters:\n",
    "        spike_trains - The spike trains with shape (num_steps, num_neurons).\n",
    "    \n",
    "    Returns:\n",
    "        decoded_vector - A tensor representing the first spike times for each neuron with gradients retained.\n",
    "    \"\"\"\n",
    "    num_steps, num_neurons = spike_trains.shape\n",
    "\n",
    "    # Create a tensor with time steps and retain gradients\n",
    "    time_tensor = torch.arange(1, num_steps + 1, dtype=torch.float32, requires_grad=True).unsqueeze(1).expand(num_steps, num_neurons)\n",
    "\n",
    "    # Multiply spike_trains by the time tensor, masking out non-spike entries\n",
    "    spike_times = spike_trains * time_tensor\n",
    "\n",
    "    # Set all zero entries (no spike) to a very high value (greater than num_steps)\n",
    "    spike_times = spike_times + (1 - spike_trains) * (num_steps+1)\n",
    "\n",
    "    # Find the minimum value in each column (i.e., first spike)\n",
    "    first_spike_times, _ = spike_times.min(dim=0)\n",
    "\n",
    "    # Ensure that this tensor retains gradients\n",
    "    return first_spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "normalized obs  tensor([0.9000, 0.6667, 0.7000, 0.0000, 0.6592, 0.0500, 0.9000, 0.6500])\n",
      "shape of spike trains tensor([[1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 0., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [0., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 0., 1., 1.],\n",
      "        [0., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 0., 1., 1.]])\n",
      "First spike times: tensor([  1.,   3.,   2., 101.,   2.,   2.,   1.,   1.])\n",
      "Spike counts: [91.0, 70.0, 71.0, 0.0, 62.0, 4.0, 92.0, 69.0]\n"
     ]
    }
   ],
   "source": [
    "# non-batch version\n",
    "observation = np.array([1.2, 0.5, 2.0, -5, 1.0, -4.5, 0.8, 0.3])  # Example observation\n",
    "threshold = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "shift = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "print(observation.shape)\n",
    "spike_trains = generate_spike_trains(observation, num_steps=100, threshold=threshold, shift=shift)\n",
    "\n",
    "spike_trains = torch.tensor(spike_trains, dtype=torch.float)\n",
    "\n",
    "print(\"shape of spike trains\",spike_trains.shape)  # [num_steps, observation_dim]\n",
    "# Get the first spike times as an array\n",
    "first_spike_times = decode_first_spike(spike_trains)\n",
    "print(\"First spike times:\", first_spike_times)\n",
    "\n",
    "# Get the spike counts as an array\n",
    "spike_counts = get_spike_counts(spike_trains)\n",
    "print(\"Spike counts:\", spike_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 100, 8])\n",
      "First spike times: tensor([[  1.,   1., 101.,   2.,   1.,  40.,   1.,   2.],\n",
      "        [  2.,   2.,   1.,   1.,   2.,   1.,   5.,   5.],\n",
      "        [  1.,   1.,   2.,   1.,   3.,   1.,   1.,   1.],\n",
      "        [  1.,   2.,   4.,   1.,   3.,   1.,   1.,   1.]])\n",
      "Spike counts: tensor([[100.,  33.,   0.,  52.,  67.,   5.,  91.,  68.],\n",
      "        [ 54.,  65.,  31.,  71.,  60.,  23.,  12.,  39.],\n",
      "        [  6.,  38.,  45.,  46.,  36.,  55.,  65.,  51.],\n",
      "        [ 68.,  62.,  47.,  48.,  54.,  48.,  50.,  56.]])\n"
     ]
    }
   ],
   "source": [
    "# batch version\n",
    "batch_observation = np.array([[1.5, -0.5, -5.0, -0.0, 1.0, -4.5, 0.8, 0.3],\n",
    "                             [0.2, 0.5, -2.0, 1.0, 0.5, -1.5, -0.8, -0.3],\n",
    "                             [-1.2, -0.5, -0.2, -0.3, -1.0, 0.4, 0.2, 0.1],\n",
    "                             [0.5, 0.5, 0.2, 0.3, 0.1, 0.0, -0.2, 0.3],\n",
    "                             [-1.5, -1.5, -5, -5, -3.14, -5, -1, -1],\n",
    "                             [1.5, 1.5, 5, 5, 3.14, 5, 1, 1]])\n",
    "\n",
    "threshold = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "shift = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "print(observation.shape)\n",
    "spike_trains = generate_spike_trains_batched(batch_observation, num_steps=100, threshold=threshold, shift=shift)\n",
    "\n",
    "spike_trains = torch.tensor(spike_trains, dtype=torch.float)\n",
    "\n",
    "print(spike_trains.shape)  # [num_steps, observation_dim]\n",
    "# Get the first spike times as an array\n",
    "first_spike_times = decode_first_spike_batched(spike_trains)\n",
    "print(\"First spike times:\", first_spike_times)\n",
    "\n",
    "# Get the spike counts as an array\n",
    "spike_counts = get_spike_counts_batched(spike_trains)\n",
    "\n",
    "print(\"Spike counts:\", spike_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
