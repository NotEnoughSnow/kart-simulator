{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import snntorch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim.adam import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    \"\"\"\n",
    "        A standard in_dim-64-64-out_dim Feed Forward Neural Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "            Initialize the network and set up the layers.\n",
    "\n",
    "            Parameters:\n",
    "                in_dim - input dimensions as an int\n",
    "                out_dim - output dimensions as an int\n",
    "\n",
    "                Return:\n",
    "                None\n",
    "        \"\"\"\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "            Runs a forward pass on the neural network.\n",
    "\n",
    "            Parameters:\n",
    "                obs - observation to pass as input\n",
    "\n",
    "            Return:\n",
    "                output - the output of our forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "\n",
    "        activation1 = torch.relu(self.layer1(obs))\n",
    "        activation2 = torch.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rtgs(batch_rews):\n",
    "    \"\"\"\n",
    "        Compute the Reward-To-Go of each timestep in a batch given the rewards.\n",
    "\n",
    "        Parameters:\n",
    "            batch_rews - the rewards in a batch, Shape: (number of episodes, number of timesteps per episode)\n",
    "\n",
    "        Return:\n",
    "            batch_rtgs - the rewards to go, Shape: (number of timesteps in batch)\n",
    "    \"\"\"\n",
    "    # The rewards-to-go (rtg) per episode per batch to return.\n",
    "    # The shape will be (num timesteps per episode)\n",
    "    batch_rtgs = []\n",
    "\n",
    "    # Iterate through each episode\n",
    "    for ep_rews in reversed(batch_rews):\n",
    "\n",
    "        discounted_reward = 0  # The discounted reward so far\n",
    "\n",
    "        # Iterate through all rewards in the episode. We go backwards for smoother calculation of each\n",
    "        # discounted return (think about why it would be harder starting from the beginning)\n",
    "        for rew in reversed(ep_rews):\n",
    "            discounted_reward = rew + discounted_reward * gamma\n",
    "            batch_rtgs.insert(0, discounted_reward)\n",
    "\n",
    "    # Convert the rewards-to-go into a tensor\n",
    "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "\n",
    "    return batch_rtgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 5\n",
    "act_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_batch = 4800  # Number of timesteps to run per batch\n",
    "max_timesteps_per_episode = 1600  # Max number of timesteps per episode\n",
    "n_updates_per_iteration = 5  # Number of times to update actor/critic per iteration\n",
    "lr = 0.005  # Learning rate of actor optimizer\n",
    "gamma = 0.95  # Discount factor to be applied when calculating Rewards-To-Go\n",
    "clip = 0.2  # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
    "\n",
    "# Miscellaneous parameters\n",
    "render = True  # If we should render during rollout\n",
    "render_every_i = 10  # Only render every n iterations\n",
    "save_freq = 10  # How often we save in number of iterations\n",
    "seed = None  # Sets the seed of our program, used for reproducibility of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance for the actor network\n",
    "actor = FeedForwardNN(obs_dim, act_dim)  # ALG STEP 1\n",
    "\n",
    "# create an instance for the critic network\n",
    "critic = FeedForwardNN(obs_dim, 1)\n",
    "\n",
    "# Initialize optimizers for actor and critic\n",
    "actor_optim = Adam(actor.parameters(), lr=lr)\n",
    "critic_optim = Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "cov_var = torch.full(size=(act_dim,), fill_value=0.5)\n",
    "cov_mat = torch.diag(cov_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first observation, from reset\n",
    "obs = torch.tensor([300.0000, 450.0000,   0.0000,   4.7124,   0.0000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  3.1073,   8.2000, -21.1976,  23.6841, -15.8548])\n"
     ]
    }
   ],
   "source": [
    "mean = actor(obs)\n",
    "\n",
    "print(mean.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.0343742   7.223215  -22.181799   24.653101  -16.455553 ]\n",
      "tensor(-6.0896)\n"
     ]
    }
   ],
   "source": [
    "# Create a distribution with the mean action and std from the covariance matrix above.\n",
    "# For more information on how this distribution works, check out Andrew Ng's lecture on it:\n",
    "# https://www.youtube.com/watch?v=JjB58InuTqM\n",
    "dist = MultivariateNormal(mean, cov_mat)\n",
    "\n",
    "# Sample an action from the distribution\n",
    "action = dist.sample()\n",
    "\n",
    "# Calculate the log probability for that action\n",
    "log_prob = dist.log_prob(action)\n",
    "\n",
    "# Return the sampled action and the log probability of that action in our distribution\n",
    "print(action.detach().numpy())\n",
    "print(log_prob.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rollout\n",
    "\n",
    "# batch obs\n",
    "batch_obs = torch.tensor([[300.0000, 450.0000,   0.0000,   4.7124,   0.0000],\n",
    "        [300.0667, 450.0000,   3.9801,   4.7124,   0.0000],\n",
    "        [300.1997, 450.0000,   7.9404,   4.7124,   0.0000],\n",
    "        [300.0000, 450.0000,   0.0000,   4.7124,   0.0000],\n",
    "        [300.0667, 450.0000,   3.9801,   4.7124,   0.0000],\n",
    "        [300.1997, 450.0000,   7.9404,   4.7124,   0.0000]])\n",
    "\n",
    "# batch actions\n",
    "batch_acts =  torch.tensor([[13.1795, 10.1534, 10.7613,  9.3591, -7.8791],\n",
    "        [13.7555,  9.6675, 10.0287,  9.6279, -6.4844],\n",
    "        [13.1954, 10.4574,  9.9011,  9.5315, -7.1549],\n",
    "        [13.4455,  9.4522,  9.5629, 10.1283, -6.6890],\n",
    "        [14.7801,  8.4991,  8.6375,  9.9466, -6.9821],\n",
    "        [13.8373, 10.2118, 10.8795,  8.7003, -6.5202]])\n",
    "\n",
    "# batch log prob\n",
    "batch_log_probs = torch.tensor([-5.0741, -3.2771, -3.4157, -4.1163, -8.0700, -4.7627])\n",
    "\n",
    "# batch rewards\n",
    "batch_rtgs = torch.tensor([-3.1173, -2.1154, -1.0746, -3.1173, -2.1154, -1.0746])\n",
    "\n",
    "# batch lengths\n",
    "batch_lens = [3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(batch_obs, batch_acts):\n",
    "    \"\"\"\n",
    "        Estimate the values of each observation, and the log probs of\n",
    "        each action in the most recent batch with the most recent\n",
    "        iteration of the actor network. Should be called from learn.\n",
    "\n",
    "        Parameters:\n",
    "            batch_obs - the observations from the most recently collected batch as a tensor.\n",
    "                        Shape: (number of timesteps in batch, dimension of observation)\n",
    "            batch_acts - the actions from the most recently collected batch as a tensor.\n",
    "                        Shape: (number of timesteps in batch, dimension of action)\n",
    "\n",
    "        Return:\n",
    "            V - the predicted values of batch_obs\n",
    "            log_probs - the log probabilities of the actions taken in batch_acts given batch_obs\n",
    "    \"\"\"\n",
    "\n",
    "    # Query critic network for a value V for each batch_obs. Shape of V should be same as batch_rtgs\n",
    "    V = critic(batch_obs).squeeze()\n",
    "\n",
    "    # Calculate the log probabilities of batch actions using most recent actor network.\n",
    "    # This segment of code is similar to that in get_action()\n",
    "    mean = actor(batch_obs)\n",
    "    dist = MultivariateNormal(mean, cov_mat)\n",
    "    log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "    # Return the value vector V of each observation in the batch\n",
    "    # and log probabilities log_probs of each action in the batch\n",
    "    return V, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.6725, 10.7427, 10.8127, 10.6725, 10.7427, 10.8127],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor([-13.7898, -12.8581, -11.8873, -13.7898, -12.8581, -11.8873])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Calculate advantage at k-th iteration\n",
    "V, _ = evaluate(batch_obs, batch_acts)\n",
    "A_k = batch_rtgs - V.detach()\n",
    "\n",
    "print(V)\n",
    "print(A_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1103, -0.0152,  1.1256, -1.1103, -0.0152,  1.1256])\n"
     ]
    }
   ],
   "source": [
    "# normalizing the advantage\n",
    "A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "print(A_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.0563, 12.1266, 12.1969, 12.0563, 12.1266, 12.1969],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor([-1398.3192, -1358.0565, -1313.4099, -1325.2916, -1275.5676, -1421.1865],\n",
      "       grad_fn=<SubBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0.], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate V_phi and pi_theta(a_t | s_t)\n",
    "V, curr_log_probs = evaluate(batch_obs, batch_acts)\n",
    "\n",
    "# Calculate the ratio pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)\n",
    "# NOTE: we just subtract the logs, which is the same as\n",
    "# dividing the values and then canceling the log with e^log.\n",
    "# For why we use log probabilities instead of actual probabilities,\n",
    "# here's a great explanation:\n",
    "# https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms\n",
    "# TL;DR makes gradient ascent easier behind the scenes.\n",
    "ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "print(V)\n",
    "print(curr_log_probs)\n",
    "print(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0., -0., 0., -0., -0., 0.], grad_fn=<MulBackward0>)\n",
      "tensor([-0.8883, -0.0122,  0.9005, -0.8883, -0.0122,  0.9005],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate surrogate losses.\n",
    "surr1 = ratios * A_k\n",
    "surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * A_k\n",
    "\n",
    "print(surr1)\n",
    "print(surr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3002, grad_fn=<MeanBackward0>)\n",
      "tensor(203.0672, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate actor and critic losses.\n",
    "# NOTE: we take the negative min of the surrogate losses because we're trying to maximize\n",
    "# the performance function, but Adam minimizes the loss. So minimizing the negative\n",
    "# performance function maximizes it.\n",
    "actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "\n",
    "print(actor_loss)\n",
    "print(critic_loss)\n",
    "\n",
    "# Calculate gradients and perform backward propagation for actor network\n",
    "actor_optim.zero_grad()\n",
    "actor_loss.backward(retain_graph=True)\n",
    "actor_optim.step()\n",
    "\n",
    "# Calculate gradients and perform backward propagation for critic network\n",
    "critic_optim.zero_grad()\n",
    "critic_loss.backward()\n",
    "critic_optim.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
