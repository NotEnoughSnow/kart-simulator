{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import snntorch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim.adam import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    \"\"\"\n",
    "        A standard in_dim-64-64-out_dim Feed Forward Neural Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "            Initialize the network and set up the layers.\n",
    "\n",
    "            Parameters:\n",
    "                in_dim - input dimensions as an int\n",
    "                out_dim - output dimensions as an int\n",
    "\n",
    "                Return:\n",
    "                None\n",
    "        \"\"\"\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "            Runs a forward pass on the neural network.\n",
    "\n",
    "            Parameters:\n",
    "                obs - observation to pass as input\n",
    "\n",
    "            Return:\n",
    "                output - the output of our forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"state\", obs)\n",
    "\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "\n",
    "        activation1 = torch.relu(self.layer1(obs))\n",
    "        activation2 = torch.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rtgs(self, batch_rews):\n",
    "    \"\"\"\n",
    "        Compute the Reward-To-Go of each timestep in a batch given the rewards.\n",
    "\n",
    "        Parameters:\n",
    "            batch_rews - the rewards in a batch, Shape: (number of episodes, number of timesteps per episode)\n",
    "\n",
    "        Return:\n",
    "            batch_rtgs - the rewards to go, Shape: (number of timesteps in batch)\n",
    "    \"\"\"\n",
    "    # The rewards-to-go (rtg) per episode per batch to return.\n",
    "    # The shape will be (num timesteps per episode)\n",
    "    batch_rtgs = []\n",
    "\n",
    "    # Iterate through each episode\n",
    "    for ep_rews in reversed(batch_rews):\n",
    "\n",
    "        discounted_reward = 0  # The discounted reward so far\n",
    "\n",
    "        # Iterate through all rewards in the episode. We go backwards for smoother calculation of each\n",
    "        # discounted return (think about why it would be harder starting from the beginning)\n",
    "        for rew in reversed(ep_rews):\n",
    "            discounted_reward = rew + discounted_reward * self.gamma\n",
    "            batch_rtgs.insert(0, discounted_reward)\n",
    "\n",
    "    # Convert the rewards-to-go into a tensor\n",
    "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "\n",
    "    return batch_rtgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 5\n",
    "act_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance for the actor network\n",
    "actor = FeedForwardNN(obs_dim, act_dim)  # ALG STEP 1\n",
    "\n",
    "# create an instance for the critic network\n",
    "\n",
    "critic = FeedForwardNN(obs_dim, 1)\n",
    "\n",
    "cov_var = torch.full(size=(act_dim,), fill_value=0.5)\n",
    "cov_mat = torch.diag(cov_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first observation, from reset\n",
    "obs = torch.tensor([300.0, 500.0, 20.0, 1.5, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state tensor([300.0000, 500.0000,  20.0000,   1.5000,   0.0000])\n",
      "tensor([  9.6540,  -1.3000,   7.4884, -37.3375, -33.8045])\n"
     ]
    }
   ],
   "source": [
    "mean = actor(obs)\n",
    "\n",
    "print(mean.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultivariateNormal(loc: torch.Size([5]), covariance_matrix: torch.Size([5, 5]))\n",
      "[  9.041698   -2.1892595   7.27935   -37.093643  -32.856956 ]\n",
      "tensor(-5.0285)\n"
     ]
    }
   ],
   "source": [
    "# Create a distribution with the mean action and std from the covariance matrix above.\n",
    "# For more information on how this distribution works, check out Andrew Ng's lecture on it:\n",
    "# https://www.youtube.com/watch?v=JjB58InuTqM\n",
    "dist = MultivariateNormal(mean, cov_mat)\n",
    "\n",
    "# Sample an action from the distribution\n",
    "action = dist.sample()\n",
    "\n",
    "# Calculate the log probability for that action\n",
    "log_prob = dist.log_prob(action)\n",
    "\n",
    "# Return the sampled action and the log probability of that action in our distribution\n",
    "print(action.detach().numpy())\n",
    "print(log_prob.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = [300.0, 500.0, 20.0, 1.5, 0.0]\n",
    "rew = 1200\n",
    "truncated = False\n",
    "terminated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch rewards\n",
    "\n",
    "# batch obs\n",
    "\n",
    "# batch actions\n",
    "\n",
    "# batch log prob\n",
    "\n",
    "# batch rewards to go\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
