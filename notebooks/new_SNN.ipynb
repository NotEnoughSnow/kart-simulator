{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch.functional as SF\n",
    "import snntorch.spikegen as spikegen\n",
    "import snntorch as snn\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.distributions import MultivariateNormal, Categorical\n",
    "\n",
    "\n",
    "hidden_size = 300  # number of hidden neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This file contains a neural network module for us to\n",
    "    define our actor and critic networks in PPO.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FFNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "        A standard in_dim-64-64-out_dim Feed Forward Neural Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "            Initialize the network and set up the layers.\n",
    "\n",
    "            Parameters:\n",
    "                in_dim - input dimensions as an int\n",
    "                out_dim - output dimensions as an int\n",
    "\n",
    "                Return:\n",
    "                None\n",
    "        \"\"\"\n",
    "        super(FFNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, out_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "            Runs a forward pass on the neural network.\n",
    "\n",
    "            Parameters:\n",
    "                obs - observation to pass as input\n",
    "\n",
    "            Return:\n",
    "                output - the output of our forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "\n",
    "\n",
    "        activation1 = torch.relu(self.layer1(obs))\n",
    "        activation2 = torch.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64  # Number of hidden neurons\n",
    "\n",
    "class SNN_small(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_steps):\n",
    "        super(SNN_small, self).__init__()\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        beta1 = 0.9\n",
    "        beta2 = torch.rand((output_size), dtype=torch.float)  # Independent decay rate for each output neuron\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, dtype=torch.float)\n",
    "        self.fc1.weight.data += 0.005\n",
    "        self.lif1 = snn.Leaky(beta=beta1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size, dtype=torch.float)\n",
    "        self.fc2.weight.data += 0.005\n",
    "        self.lif2 = snn.Leaky(beta=beta2, learn_beta=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        # Determine if input is batched or not\n",
    "        is_batched = x.dim() == 3  # [batch_size, num_steps, input_size] is 3D\n",
    "\n",
    "        if not is_batched:\n",
    "            # If not batched, add a batch dimension\n",
    "            x = x.unsqueeze(0)  # Shape becomes [1, num_steps, input_size]\n",
    "\n",
    "\n",
    "        batch_size = x.size(0)  # This is 1 if not batched, otherwise the actual batch size\n",
    "\n",
    "        # Initialize membrane potentials\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the spikes from the last layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "\n",
    "        output_spk = torch.stack(spk2_rec, dim=1)  # Shape: [batch_size, num_steps, output_size]\n",
    "        output_mem = torch.stack(mem2_rec, dim=1)  # Shape: [batch_size, num_steps, output_size]\n",
    "\n",
    "        if not is_batched:\n",
    "            # Remove the batch dimension if it was added\n",
    "            output_spk = output_spk.squeeze(0)  # Shape becomes [num_steps, output_size]\n",
    "            output_mem = output_mem.squeeze(0)  # Shape becomes [num_steps, output_size]\n",
    "\n",
    "        #print(\"should not be none :\", output_spk.grad_fn)  # This should not be None\n",
    "\n",
    "        return output_spk, output_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300  # Number of hidden neurons\n",
    "\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_steps):\n",
    "        super(SNN, self).__init__()\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.9\n",
    "        beta3 = torch.rand((output_size), dtype=torch.float)  # Independent decay rate for each output neuron\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, dtype=torch.float)\n",
    "        self.fc1.weight.data += 0.005\n",
    "        self.lif1 = snn.Leaky(beta=beta1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float)\n",
    "        self.fc2.weight.data += 0.005\n",
    "        self.lif2 = snn.Leaky(beta=beta2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size, dtype=torch.float)\n",
    "        self.fc3.weight.data += 0.005\n",
    "        self.lif3 = snn.Leaky(beta=beta3, learn_beta=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        # Determine if input is batched or not\n",
    "        is_batched = x.dim() == 3  # [batch_size, num_steps, input_size] is 3D\n",
    "\n",
    "        if not is_batched:\n",
    "            # If not batched, add a batch dimension\n",
    "            x = x.unsqueeze(0)  # Shape becomes [1, num_steps, input_size]\n",
    "\n",
    "\n",
    "        batch_size = x.size(0)  # This is 1 if not batched, otherwise the actual batch size\n",
    "\n",
    "        # Initialize membrane potentials\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        # Record the spikes from the last layer\n",
    "        spk3_rec = []\n",
    "        mem3_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "\n",
    "        output_spk = torch.stack(spk3_rec, dim=1)  # Shape: [batch_size, num_steps, output_size]\n",
    "        output_mem = torch.stack(mem3_rec, dim=1)  # Shape: [batch_size, num_steps, output_size]\n",
    "\n",
    "        if not is_batched:\n",
    "            # Remove the batch dimension if it was added\n",
    "            output_spk = output_spk.squeeze(0)  # Shape becomes [num_steps, output_size]\n",
    "            output_mem = output_mem.squeeze(0)  # Shape becomes [num_steps, output_size]\n",
    "\n",
    "        return output_spk, output_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spike_trains(observation, num_steps, threshold, shift):\n",
    "    \"\"\"\n",
    "    Generate spike trains from a single observation using a fixed global threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - observation: A tensor representing the observation ([observation_dim]).\n",
    "    - num_steps: The number of timesteps for the spike train.\n",
    "    - threshold: A single global threshold value to be used for normalization.\n",
    "    \n",
    "    Returns:\n",
    "    - spike_trains: Tensor of spike trains.\n",
    "    \"\"\"\n",
    "    \n",
    "    shift = shift.numpy()\n",
    "\n",
    "    # Normalize and clip observation\n",
    "    shifted_obs = np.add(observation, shift) \n",
    "\n",
    "    # torch version\n",
    "    #shifted_obs = observation + shift\n",
    "\n",
    "\n",
    "    normalized_obs = shifted_obs / (threshold + 1e-6)  # Avoid division by zero\n",
    "\n",
    "    normalized_obs /= 2\n",
    "    \n",
    "    normalized_obs = normalized_obs.clamp(0, 1)  # Clip values to be within [0, 1]\n",
    "\n",
    "    \n",
    "    # Generate spike trains\n",
    "    spike_trains = spikegen.rate(normalized_obs, num_steps=num_steps)\n",
    "    \n",
    "    # torch version\n",
    "    #return spike_trains\n",
    "\n",
    "    return spike_trains.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spike_trains_batched(observations, num_steps, threshold, shift):\n",
    "    \"\"\"\n",
    "    Generate spike trains from batched observations using a fixed global threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - observations: A tensor representing the batched observations ([batch_size, observation_dim]).\n",
    "    - num_steps: The number of timesteps for the spike train.\n",
    "    - threshold: A single global threshold value to be used for normalization.\n",
    "    - shift: A value to shift the observation range to handle negative values.\n",
    "    \n",
    "    Returns:\n",
    "    - spike_trains: Tensor of spike trains with shape (batch_size, num_steps, observation_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    shift = shift.numpy()\n",
    "\n",
    "\n",
    "    # Normalize and shift observations\n",
    "    normalized_obs = np.add(observations, shift) / (2 * (threshold + 1e-6))  # Avoid division by zero\n",
    "    normalized_obs = normalized_obs.clamp(0, 1)  # Clip values to [0, 1]\n",
    "\n",
    "    # Generate spike trains for each observation in the batch\n",
    "    spike_trains = spikegen.rate(normalized_obs, num_steps=num_steps)\n",
    "    \n",
    "    # Rearrange the output to have shape (batch_size, num_steps, observation_dim)\n",
    "    spike_trains = spike_trains.permute(1, 0, 2)\n",
    "    \n",
    "    # torch version\n",
    "    #return spike_trains\n",
    "\n",
    "    return spike_trains.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spike_counts(spike_trains):\n",
    "    \"\"\"\n",
    "    Get the total number of spikes for each neuron over all timesteps.\n",
    "    \n",
    "    Parameters:\n",
    "    - spike_trains: Tensor of spike trains with shape [num_steps, observation_dim].\n",
    "    \n",
    "    Returns:\n",
    "    - Array of spike counts for each neuron.\n",
    "    \"\"\"\n",
    "\n",
    "    num_steps, num_neurons = spike_trains.shape\n",
    "\n",
    "\n",
    "    spike_counts = torch.sum(spike_trains, dim=0)\n",
    "\n",
    "    spike_counts = spike_counts/num_steps\n",
    "\n",
    "    return spike_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spike_counts_batched(spike_trains):\n",
    "    \"\"\"\n",
    "    Get the total number of spikes for each neuron over all timesteps for batched spike trains.\n",
    "    \n",
    "    Parameters:\n",
    "    - spike_trains: Tensor of spike trains with shape [batch_size, num_steps, observation_dim].\n",
    "    \n",
    "    Returns:\n",
    "    - Array of spike counts for each neuron in each observation (shape: [batch_size, observation_dim]).\n",
    "    \"\"\"\n",
    "    batch_size, num_steps, num_neurons = spike_trains.shape\n",
    "\n",
    "\n",
    "    # Sum over the time dimension (dim=1) to get spike counts for each neuron in each observation\n",
    "    spike_counts = torch.sum(spike_trains, dim=1)\n",
    "    \n",
    "    spike_counts = spike_counts/num_steps\n",
    "\n",
    "\n",
    "    return spike_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_first_spike_batched(spike_trains):\n",
    "    \"\"\"\n",
    "    Decodes the first spike time from batched spike trains using the 'time to first spike' method.\n",
    "    \n",
    "    Parameters:\n",
    "        spike_trains - The batched spike trains with shape (batch_size, num_steps, num_neurons).\n",
    "    \n",
    "    Returns:\n",
    "        decoded_vector - A tensor representing the first spike times for each neuron in each batch with gradients retained.\n",
    "    \"\"\"\n",
    "    batch_size, num_steps, num_neurons = spike_trains.shape\n",
    "\n",
    "    # Create a tensor with time steps and retain gradients\n",
    "    time_tensor = torch.arange(1, num_steps + 1, dtype=torch.float32, requires_grad=True).unsqueeze(0).unsqueeze(2).expand(batch_size, num_steps, num_neurons)\n",
    "\n",
    "    # Multiply spike_trains by the time tensor, masking out non-spike entries\n",
    "    spike_times = spike_trains * time_tensor\n",
    "\n",
    "    # Set all zero entries (no spike) to a very high value (greater than num_steps)\n",
    "    spike_times = spike_times + (1 - spike_trains) * (num_steps+1)\n",
    "\n",
    "    # Find the minimum value in each column (i.e., first spike) for each batch\n",
    "    first_spike_times, _ = spike_times.min(dim=1)\n",
    "\n",
    "    # Transform the spike times into a format better suited for a categorical \n",
    "    first_spike_times = (-2/(num_steps+1))*first_spike_times + 2\n",
    "\n",
    "\n",
    "    # Ensure that this tensor retains gradients\n",
    "    return first_spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_first_spike(spike_trains):\n",
    "    \"\"\"\n",
    "    Decodes the first spike time from spike trains using the 'time to first spike' method.\n",
    "    \n",
    "    Parameters:\n",
    "        spike_trains - The spike trains with shape (num_steps, num_neurons).\n",
    "    \n",
    "    Returns:\n",
    "        decoded_vector - A tensor representing the first spike times for each neuron with gradients retained.\n",
    "    \"\"\"\n",
    "    num_steps, num_neurons = spike_trains.shape\n",
    "\n",
    "    # Create a tensor with time steps and retain gradients\n",
    "    time_tensor = torch.arange(1, num_steps + 1, dtype=torch.float32, requires_grad=True).unsqueeze(1).expand(num_steps, num_neurons)\n",
    "\n",
    "    # Multiply spike_trains by the time tensor, masking out non-spike entries\n",
    "    spike_times = spike_trains * time_tensor\n",
    "\n",
    "    # Set all zero entries (no spike) to a very high value (greater than num_steps)\n",
    "    spike_times = spike_times + (1 - spike_trains) * (num_steps+1)\n",
    "\n",
    "    # Find the minimum value in each column (i.e., first spike)\n",
    "    first_spike_times, _ = spike_times.min(dim=0)\n",
    "\n",
    "    # Transform the spike times into a format better suited for a categorical \n",
    "    first_spike_times = (-2/(num_steps+1))*first_spike_times + 2\n",
    "\n",
    "\n",
    "    # Ensure that this tensor retains gradients\n",
    "    return first_spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_first_spike_batched_archived(spike_trains):\n",
    "    \"\"\"\n",
    "    Decodes the first spike time from spike trains for batched data using 'time to first spike' method.\n",
    "\n",
    "    Parameters:\n",
    "        spike_trains - The batched spike trains with shape (batch_size, num_steps, num_neurons).\n",
    "\n",
    "    Returns:\n",
    "        decoded_vector - The decoded first spike times with shape (batch_size, num_neurons).\n",
    "    \"\"\"\n",
    "    batch_size = spike_trains.size(0)\n",
    "    num_neurons = spike_trains.size(2)\n",
    "    decoded_vectors = []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        decoded_vector = [spike_trains.size(1)+1] * num_neurons\n",
    "        \n",
    "        for neuron_idx in range(num_neurons):\n",
    "            first_spike = (spike_trains[batch_idx, :, neuron_idx] == 1).nonzero(as_tuple=True)[0]\n",
    "            if first_spike.nelement() != 0:\n",
    "                decoded_vector[neuron_idx] = first_spike[0].item() + 1\n",
    "        \n",
    "        decoded_vectors.append(decoded_vector)\n",
    "\n",
    "    return torch.FloatTensor(decoded_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_first_spike_archived(spike_trains):\n",
    "    \"\"\"\n",
    "    Decodes the first spike time from spike trains using the 'time to first spike' method for non-batched data.\n",
    "\n",
    "    Parameters:\n",
    "        spike_trains - The spike trains with shape (num_steps, num_neurons).\n",
    "\n",
    "    Returns:\n",
    "        decoded_vector - The decoded first spike times with shape (num_neurons,).\n",
    "    \"\"\"\n",
    "    num_steps = spike_trains.size(0)\n",
    "    num_neurons = spike_trains.size(1)\n",
    "    \n",
    "    # Initialize decoded vector with default values greater than the maximum possible spike time\n",
    "    decoded_vector = [num_steps + 1] * num_neurons\n",
    "\n",
    "    # Iterate over each neuron to find the first spike time\n",
    "    for neuron_idx in range(num_neurons):\n",
    "        first_spike = (spike_trains[:, neuron_idx] == 1).nonzero(as_tuple=True)[0]\n",
    "        if first_spike.nelement() != 0:\n",
    "            decoded_vector[neuron_idx] = first_spike[0].item() + 1  # +1 to convert 0-based index to 1-based time step\n",
    "\n",
    "    return torch.FloatTensor(decoded_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_count(spikes):\n",
    "    spike_counts = torch.sum(spikes, dim=1)\n",
    "    action = torch.zeros(spikes.size(0))\n",
    "    max_spike_count = torch.max(spike_counts)\n",
    "    candidates = torch.where(spike_counts == max_spike_count)[0]\n",
    "    if len(candidates) > 1:\n",
    "        action[torch.multinomial(candidates.float(), 1)] = 1\n",
    "    else:\n",
    "        action[candidates] = 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "shape of spike trains torch.Size([100, 8])\n",
      "First spike times: tensor([1.9802, 1.9604, 1.9802, 0.0000, 1.9802, 1.6832, 1.9802, 1.9604],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Spike counts: tensor([0.8600, 0.6600, 0.7500, 0.0000, 0.5900, 0.0400, 0.9100, 0.6200])\n"
     ]
    }
   ],
   "source": [
    "# non-batch version\n",
    "observation = np.array([1.2, 0.5, 2.0, -5, 1.0, -4.5, 0.8, 0.3])  # Example observation\n",
    "threshold = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "shift = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "print(observation.shape)\n",
    "spike_trains = generate_spike_trains(observation, num_steps=100, threshold=threshold, shift=shift)\n",
    "\n",
    "spike_trains = torch.tensor(spike_trains, dtype=torch.float)\n",
    "\n",
    "print(\"shape of spike trains\",spike_trains.shape)  # [num_steps, observation_dim]\n",
    "# Get the first spike times as an array\n",
    "first_spike_times = decode_first_spike(spike_trains)\n",
    "print(\"First spike times:\", first_spike_times)\n",
    "\n",
    "# Get the spike counts as an array\n",
    "spike_counts = get_spike_counts(spike_trains)\n",
    "print(\"Spike counts:\", spike_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "torch.Size([6, 100, 8])\n",
      "First spike times: tensor([[1.9802, 1.9208, 0.0000, 1.9604, 1.9802, 1.6238, 1.9604, 1.9802],\n",
      "        [1.9406, 1.9604, 1.9604, 1.9802, 1.9802, 1.9604, 1.6832, 1.9604],\n",
      "        [1.7426, 1.9604, 1.9802, 1.9604, 1.9802, 1.9802, 1.9802, 1.9802],\n",
      "        [1.9802, 1.9802, 1.9802, 1.9604, 1.9802, 1.9802, 1.9802, 1.9604],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.9802, 1.9802, 1.9802, 1.9802, 1.9802, 1.9802, 1.9802, 1.9802]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Spike counts: tensor([[1.0000, 0.3100, 0.0000, 0.5200, 0.6600, 0.0700, 0.8800, 0.6700],\n",
      "        [0.4800, 0.7300, 0.3000, 0.5700, 0.6200, 0.3400, 0.0900, 0.3500],\n",
      "        [0.0700, 0.3400, 0.4600, 0.5000, 0.3400, 0.5700, 0.5300, 0.6100],\n",
      "        [0.7300, 0.6700, 0.5300, 0.4800, 0.5000, 0.4700, 0.4200, 0.5200],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# batch version\n",
    "batch_observation = np.array([[1.5, -0.5, -5.0, -0.0, 1.0, -4.5, 0.8, 0.3],\n",
    "                             [0.2, 0.5, -2.0, 1.0, 0.5, -1.5, -0.8, -0.3],\n",
    "                             [-1.2, -0.5, -0.2, -0.3, -1.0, 0.4, 0.2, 0.1],\n",
    "                             [0.5, 0.5, 0.2, 0.3, 0.1, 0.0, -0.2, 0.3],\n",
    "                             [-1.5, -1.5, -5, -5, -3.14, -5, -1, -1],\n",
    "                             [1.5, 1.5, 5, 5, 3.14, 5, 1, 1]])\n",
    "\n",
    "threshold = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "shift = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "print(observation.shape)\n",
    "spike_trains = generate_spike_trains_batched(batch_observation, num_steps=100, threshold=threshold, shift=shift)\n",
    "\n",
    "spike_trains = torch.tensor(spike_trains, dtype=torch.float)\n",
    "\n",
    "print(spike_trains.shape)  # [num_steps, observation_dim]\n",
    "# Get the first spike times as an array\n",
    "first_spike_times = decode_first_spike_batched(spike_trains)\n",
    "print(\"First spike times:\", first_spike_times)\n",
    "\n",
    "# Get the spike counts as an array\n",
    "spike_counts = get_spike_counts_batched(spike_trains)\n",
    "\n",
    "print(\"Spike counts:\", spike_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNN code rundown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 50\n",
    "\n",
    "obs_dim = 8\n",
    "act_dim = 4\n",
    "\n",
    "actor_SNN = SNN_small(obs_dim, act_dim, num_steps)\n",
    "critic_SNN = SNN_small(obs_dim, 1, num_steps)\n",
    "\n",
    "actor_ANN = FFNetwork(obs_dim, act_dim)\n",
    "critic_ANN = FFNetwork(obs_dim, 1)\n",
    "\n",
    "cov_var = torch.full(size=(act_dim,), fill_value=0.5)\n",
    "cov_mat = torch.diag(cov_var)\n",
    "\n",
    "threshold = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "shift = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making sure the data is well distributed after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 8)\n",
      "obs spike trains shape torch.Size([11, 50, 8])\n",
      "obs First spike times: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.9608, 0.0000, 1.8431, 1.6863, 1.4510, 1.4118, 1.9608, 1.9608],\n",
      "        [1.9216, 1.5686, 1.9216, 1.8431, 1.9608, 1.8824, 1.9608, 0.0000],\n",
      "        [1.9216, 1.7255, 1.9608, 1.9608, 1.9608, 1.9216, 1.9216, 0.0000],\n",
      "        [1.9608, 1.9608, 1.9608, 1.9608, 1.9608, 1.9608, 1.7255, 0.0000],\n",
      "        [0.0000, 0.0000, 1.8824, 1.7647, 1.2549, 0.7843, 1.9608, 0.0000],\n",
      "        [1.9608, 0.0000, 1.9608, 1.0980, 1.9608, 1.9608, 0.0000, 0.0000],\n",
      "        [1.9608, 1.9608, 1.9608, 1.8824, 1.9216, 1.9216, 0.0000, 0.0000],\n",
      "        [1.9608, 1.9608, 1.9608, 1.8039, 0.7059, 1.9608, 0.0000, 0.0000],\n",
      "        [1.9608, 1.9608, 1.4902, 1.4902, 0.0000, 1.8039, 1.9216, 1.9216],\n",
      "        [1.9608, 1.9608, 1.9608, 1.9608, 1.9608, 1.9608, 1.8824, 1.9608]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "obs Spike counts: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0400, 0.0000, 0.1400, 0.1400, 0.0400, 0.1400, 0.4000, 0.5000],\n",
      "        [0.0600, 0.1400, 0.3000, 0.2200, 0.1400, 0.2400, 0.5200, 0.0000],\n",
      "        [0.0800, 0.1800, 0.5600, 0.4400, 0.2600, 0.6600, 0.4200, 0.0000],\n",
      "        [0.9200, 0.9000, 0.7200, 0.6600, 0.7200, 0.9200, 0.6000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4800, 0.3600, 0.1200, 0.0800, 0.5200, 0.0000],\n",
      "        [0.9000, 0.0000, 0.8200, 0.0200, 0.7400, 0.1600, 0.0000, 0.0000],\n",
      "        [0.9000, 0.7600, 0.5200, 0.5800, 0.4600, 0.8200, 0.0000, 0.0000],\n",
      "        [1.0000, 0.9400, 0.8400, 0.1400, 0.0200, 0.7000, 0.0000, 0.0000],\n",
      "        [0.9600, 1.0000, 0.1000, 0.0400, 0.0000, 0.1400, 0.6600, 0.5800],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.4600, 0.4800]])\n",
      "///////////////////////////////////\n",
      "action spike trains shape torch.Size([11, 50, 4])\n",
      "action First spike times: tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6471, 0.0000],\n",
      "        [0.5098, 0.0000, 1.7647, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.7647, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6863, 0.0000],\n",
      "        [0.0000, 0.0000, 1.7255, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6863, 0.0000],\n",
      "        [1.6078, 0.0000, 1.8039, 0.0000]], grad_fn=<AddBackward0>)\n",
      "action Spike counts: tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0600, 0.0000],\n",
      "        [0.0400, 0.0000, 0.2000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2200, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1800, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1600, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1800, 0.0000],\n",
      "        [0.1000, 0.0000, 0.2600, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "batch_observation = np.array([[-1.5, -1.5, -5, -5, -3.14, -5, 0, 0],\n",
    "                              [-1.4, -1.4, -4, -4, -3.0, -4, 1, 1],\n",
    "                              [-1.2, -1.2, -2, -2, -2.5, -2, 1, 0],\n",
    "                              [-1.0, -1.0, -0, -0, -1.5, 2, 1, 0],\n",
    "                              [1.2, 1.3, 2, 2, 2.1, 4, 1, 0],\n",
    "\n",
    "                              [-1.4, -1.5, -0, -2, -3.0, -4, 1, 0],\n",
    "                              [1.2, -1.4, 4, -4, 1.12, -4, 0, 0],\n",
    "                              [1.1, 1.1, 0, 1, 0.2, 2, 0, 0],\n",
    "                              [1.5, 1.3, 3, -4, -3.0, 1, 0.0, 0.0],\n",
    "                              [1.4, 1.4, -4, -4, -3.0, -4, 1, 1],\n",
    "                             [1.5, 1.5, 5, 5, 3.14, 5, 1, 1]])\n",
    "\n",
    "print(batch_observation.shape)\n",
    "batch_observation_st = generate_spike_trains_batched(batch_observation, num_steps=50, threshold=threshold, shift=shift)\n",
    "batch_observation_st_tensor = torch.tensor(batch_observation_st, dtype=torch.float)\n",
    "\n",
    "print(\"obs spike trains shape\", batch_observation_st_tensor.shape)  # [num_steps, observation_dim]\n",
    "print(\"obs First spike times:\", decode_first_spike_batched(batch_observation_st_tensor))\n",
    "print(\"obs Spike counts:\", get_spike_counts_batched(batch_observation_st_tensor))\n",
    "\n",
    "action = actor_SNN(batch_observation_st)[0].detach()\n",
    "\n",
    "print(\"///////////////////////////////////\")\n",
    "\n",
    "print(\"action spike trains shape\", action.shape)  # [num_steps, observation_dim]\n",
    "print(\"action First spike times:\", decode_first_spike_batched(action))\n",
    "print(\"action Spike counts:\", get_spike_counts_batched(action))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 50, 8])\n",
      "obs first spikes: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8431, 1.9608],\n",
      "        [1.8039, 1.0196, 1.3725, 1.8824, 0.0000, 1.6471, 1.9608, 1.9608],\n",
      "        [1.4118, 1.6471, 1.9216, 1.8431, 1.6863, 1.9608, 1.9608, 1.8039],\n",
      "        [1.9608, 1.9216, 1.9608, 1.9608, 1.8431, 1.9608, 1.9608, 1.9608],\n",
      "        [1.8824, 1.9608, 1.9608, 1.9608, 1.9216, 1.9608, 1.9608, 1.9216],\n",
      "        [1.8039, 0.0000, 1.9608, 1.9216, 0.0000, 1.4118, 1.9608, 1.9216],\n",
      "        [1.9608, 0.5490, 1.9608, 1.6863, 1.9608, 1.9608, 1.9608, 1.9216],\n",
      "        [1.9608, 1.9608, 1.8824, 1.9608, 1.9608, 1.9216, 1.8824, 1.9216],\n",
      "        [1.9608, 1.9608, 1.9608, 1.9608, 1.4902, 1.9608, 1.9608, 1.8824],\n",
      "        [1.9608, 1.9608, 1.4510, 1.8431, 1.8039, 1.8431, 1.9608, 1.9608],\n",
      "        [1.9608, 1.9608, 1.9608, 1.9608, 1.9608, 1.9608, 1.9608, 1.9608]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "obs spike counts: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4600, 0.4800],\n",
      "        [0.0400, 0.0400, 0.1400, 0.2400, 0.0000, 0.0800, 1.0000, 1.0000],\n",
      "        [0.1400, 0.0400, 0.2600, 0.2000, 0.1600, 0.3400, 1.0000, 0.4600],\n",
      "        [0.2200, 0.1600, 0.4200, 0.5200, 0.2800, 0.7600, 1.0000, 0.4800],\n",
      "        [0.9200, 0.9200, 0.7400, 0.6600, 0.7800, 0.9000, 1.0000, 0.4200],\n",
      "        [0.0800, 0.0000, 0.5000, 0.3800, 0.0000, 0.0600, 1.0000, 0.4000],\n",
      "        [0.8400, 0.0400, 0.9000, 0.1000, 0.6600, 0.1400, 0.4800, 0.5000],\n",
      "        [0.8400, 0.9000, 0.4800, 0.5800, 0.5600, 0.6800, 0.3600, 0.4600],\n",
      "        [1.0000, 0.9600, 0.7600, 0.1200, 0.0200, 0.7200, 0.5200, 0.4600],\n",
      "        [0.9400, 0.9600, 0.1200, 0.1200, 0.0800, 0.1200, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n",
      "torch.Size([11, 50, 4])\n",
      "action first spikes: tensor([[0.0000, 0.0000, 0.6275, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6471, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0980, 0.0000],\n",
      "        [0.0000, 0.0000, 1.5686, 0.0000],\n",
      "        [0.7059, 0.0000, 1.6863, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1765, 0.0000],\n",
      "        [0.0000, 0.0000, 1.7647, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6863, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6471, 0.0000],\n",
      "        [0.3529, 0.0000, 1.8039, 0.0000],\n",
      "        [1.8431, 0.0000, 1.8039, 0.0000]], grad_fn=<AddBackward0>)\n",
      "action spike counts: tensor([[0.0000, 0.0000, 0.0200, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0600, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000],\n",
      "        [0.0200, 0.0000, 0.2200, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0400, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1800, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1600, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1600, 0.0000],\n",
      "        [0.0200, 0.0000, 0.2200, 0.0000],\n",
      "        [0.1600, 0.0000, 0.2400, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "batch_obs_st = generate_spike_trains_batched(batch_observation, num_steps=num_steps, threshold=threshold, shift=threshold)\n",
    "\n",
    "batch_obs_st = torch.tensor(batch_obs_st, dtype=torch.float)\n",
    "\n",
    "\n",
    "print(batch_obs_st.shape)\n",
    "print(\"obs first spikes:\", decode_first_spike_batched(batch_obs_st))\n",
    "print(\"obs spike counts:\", get_spike_counts_batched(batch_obs_st))\n",
    "\n",
    "\n",
    "action = actor_SNN(batch_obs_st)[0].detach()\n",
    "\n",
    "print(action.shape)\n",
    "\n",
    "print(\"action first spikes:\", decode_first_spike_batched(action))\n",
    "print(\"action spike counts:\", get_spike_counts_batched(action))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 8])\n",
      "obs first spikes: tensor([1.9608, 1.9608, 1.9216, 0.0000, 1.9608, 1.8039, 1.9608, 1.9608],\n",
      "       grad_fn=<AddBackward0>)\n",
      "obs spike counts: tensor([0.9000, 0.7200, 0.7400, 0.0000, 0.7800, 0.0600, 0.9600, 0.4600])\n",
      "torch.Size([50, 4])\n",
      "action first spikes: tensor([0.0000, 0.0000, 1.7255, 0.0000], grad_fn=<AddBackward0>)\n",
      "action spike counts: tensor([0.0000, 0.0000, 0.2200, 0.0000], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "obs_st= generate_spike_trains(observation, num_steps=num_steps, threshold=threshold, shift=threshold)\n",
    "\n",
    "obs_st = torch.tensor(obs_st, dtype=torch.float)\n",
    "\n",
    "print(obs_st.shape)\n",
    "print(\"obs first spikes:\", decode_first_spike(obs_st))\n",
    "print(\"obs spike counts:\", get_spike_counts(obs_st))\n",
    "\n",
    "\n",
    "action = actor_SNN(obs_st)[0]\n",
    "\n",
    "assert action.shape == torch.Size([50, 4])\n",
    "\n",
    "print(action.shape)\n",
    "\n",
    "print(\"action first spikes:\", decode_first_spike(action))\n",
    "print(\"action spike counts:\", get_spike_counts(action))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "V_st = critic_SNN(batch_obs_st)[0]\n",
    "V = decode_first_spike_batched(V_st).squeeze()\n",
    "\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5677, -0.4693, -0.3114, -0.0203,  0.0304, -0.3279, -0.1077,  0.0247,\n",
      "        -0.1259, -0.4444, -0.0196], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "V = critic_ANN(batch_observation).squeeze()\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.4811e-01,  2.8154e-01, -1.0281e-02,  3.6576e-01],\n",
      "        [ 6.2320e-01,  2.9533e-01, -7.3855e-02,  3.7320e-01],\n",
      "        [ 4.0395e-01,  1.9251e-01,  1.4186e-02,  2.3039e-01],\n",
      "        [ 1.8126e-01, -1.1135e-01, -2.7267e-02,  8.0497e-02],\n",
      "        [ 4.8094e-01,  1.0195e-02, -8.2545e-04,  1.4665e-01],\n",
      "        [ 4.9803e-01,  1.7471e-01, -2.8065e-02,  8.1069e-02],\n",
      "        [ 5.1968e-01,  1.5740e-01,  1.3518e-01, -5.4922e-02],\n",
      "        [ 2.4487e-01, -3.3243e-02,  5.5973e-04,  1.0340e-01],\n",
      "        [ 3.1853e-01,  7.3071e-02,  1.2470e-01,  4.1841e-02],\n",
      "        [ 5.7527e-01,  2.4985e-01, -1.1072e-01,  2.5596e-01],\n",
      "        [ 7.7228e-01,  1.6018e-01,  1.0334e-02,  1.2687e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([11, 4]), covariance_matrix: torch.Size([11, 4, 4]))\n"
     ]
    }
   ],
   "source": [
    "mean = actor_ANN(batch_observation)\n",
    "dist = MultivariateNormal(mean, cov_mat)\n",
    "\n",
    "print(mean)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.6275, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6471, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0980, 0.0000],\n",
      "        [0.0000, 0.0000, 1.5686, 0.0000],\n",
      "        [0.7059, 0.0000, 1.6863, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1765, 0.0000],\n",
      "        [0.0000, 0.0000, 1.7647, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6863, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6471, 0.0000],\n",
      "        [0.3529, 0.0000, 1.8039, 0.0000],\n",
      "        [1.8431, 0.0000, 1.8039, 0.0000]], grad_fn=<AddBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([11, 4]), covariance_matrix: torch.Size([11, 4, 4]))\n"
     ]
    }
   ],
   "source": [
    "mean_st = actor_SNN(batch_obs_st)[0]\n",
    "mean = decode_first_spike_batched(mean_st)\n",
    "dist = MultivariateNormal(mean, cov_mat)\n",
    "print(mean)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.4811e-01,  2.8154e-01, -1.0281e-02,  3.6576e-01],\n",
      "        [ 6.2320e-01,  2.9533e-01, -7.3855e-02,  3.7320e-01],\n",
      "        [ 4.0395e-01,  1.9251e-01,  1.4186e-02,  2.3039e-01],\n",
      "        [ 1.8126e-01, -1.1135e-01, -2.7267e-02,  8.0497e-02],\n",
      "        [ 4.8094e-01,  1.0195e-02, -8.2545e-04,  1.4665e-01],\n",
      "        [ 4.9803e-01,  1.7471e-01, -2.8065e-02,  8.1069e-02],\n",
      "        [ 5.1968e-01,  1.5740e-01,  1.3518e-01, -5.4922e-02],\n",
      "        [ 2.4487e-01, -3.3243e-02,  5.5973e-04,  1.0340e-01],\n",
      "        [ 3.1853e-01,  7.3071e-02,  1.2470e-01,  4.1841e-02],\n",
      "        [ 5.7527e-01,  2.4985e-01, -1.1072e-01,  2.5596e-01],\n",
      "        [ 7.7228e-01,  1.6018e-01,  1.0334e-02,  1.2687e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Categorical(logits: torch.Size([11, 4]))\n",
      "tensor([0, 1, 3, 1, 2, 0, 2, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "logits = actor_ANN(batch_observation)\n",
    "dist = Categorical(logits=logits)\n",
    "\n",
    "print(logits)\n",
    "print(dist)\n",
    "\n",
    "print(dist.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.6275, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6471, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0980, 0.0000],\n",
      "        [0.0000, 0.0000, 1.5686, 0.0000],\n",
      "        [0.7059, 0.0000, 1.6863, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1765, 0.0000],\n",
      "        [0.0000, 0.0000, 1.7647, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6863, 0.0000],\n",
      "        [0.0000, 0.0000, 1.6471, 0.0000],\n",
      "        [0.3529, 0.0000, 1.8039, 0.0000],\n",
      "        [1.8431, 0.0000, 1.8039, 0.0000]], grad_fn=<AddBackward0>)\n",
      "Categorical(logits: torch.Size([11, 4]))\n",
      "percentages  tensor([[0.2052, 0.2052, 0.3843, 0.2052],\n",
      "        [0.1221, 0.1221, 0.6338, 0.1221],\n",
      "        [0.1667, 0.1667, 0.4999, 0.1667],\n",
      "        [0.1282, 0.1282, 0.6154, 0.1282],\n",
      "        [0.2149, 0.1061, 0.5729, 0.1061],\n",
      "        [0.1602, 0.1602, 0.5195, 0.1602],\n",
      "        [0.1131, 0.1131, 0.6606, 0.1131],\n",
      "        [0.1191, 0.1191, 0.6428, 0.1191],\n",
      "        [0.1221, 0.1221, 0.6338, 0.1221],\n",
      "        [0.1499, 0.1053, 0.6395, 0.1053],\n",
      "        [0.4389, 0.0695, 0.4221, 0.0695]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([2, 2, 1, 2, 0, 2, 3, 2, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "logits_st = actor_SNN(batch_obs_st)[0]\n",
    "logits = decode_first_spike_batched(logits_st)\n",
    "\n",
    "dist = Categorical(logits=logits)\n",
    "\n",
    "print(logits)\n",
    "print(dist)\n",
    "\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "print(\"percentages \", m(logits))\n",
    "\n",
    "print(dist.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[0.0000, 0.0200, 0.0000, 0.0000],\n",
      "        [0.0600, 0.1000, 0.0000, 0.0000],\n",
      "        [0.0600, 0.0600, 0.0000, 0.0000],\n",
      "        [0.1000, 0.0600, 0.0000, 0.0000],\n",
      "        [0.1000, 0.1200, 0.0000, 0.0000],\n",
      "        [0.0400, 0.0600, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0400, 0.0000, 0.0000],\n",
      "        [0.0600, 0.1200, 0.0000, 0.0000],\n",
      "        [0.0200, 0.0600, 0.0000, 0.0000],\n",
      "        [0.0600, 0.1400, 0.0000, 0.0000],\n",
      "        [0.1200, 0.2600, 0.0000, 0.0000]], grad_fn=<DivBackward0>)\n",
      "percentages  tensor([[0.2487, 0.2538, 0.2487, 0.2487],\n",
      "        [0.2548, 0.2652, 0.2400, 0.2400],\n",
      "        [0.2575, 0.2575, 0.2425, 0.2425],\n",
      "        [0.2652, 0.2548, 0.2400, 0.2400],\n",
      "        [0.2611, 0.2664, 0.2363, 0.2363],\n",
      "        [0.2537, 0.2588, 0.2437, 0.2437],\n",
      "        [0.2475, 0.2576, 0.2475, 0.2475],\n",
      "        [0.2535, 0.2691, 0.2387, 0.2387],\n",
      "        [0.2499, 0.2601, 0.2450, 0.2450],\n",
      "        [0.2521, 0.2731, 0.2374, 0.2374],\n",
      "        [0.2548, 0.2931, 0.2260, 0.2260]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([2, 1, 1, 1, 2, 1, 1, 0, 2, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "logits_st = actor_SNN(batch_obs_st)[0]\n",
    "\n",
    "spike_counts = get_spike_counts_batched(logits_st)\n",
    "\n",
    "print(\"logits:\", logits)\n",
    "\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "print(\"percentages \", m(logits))\n",
    "\n",
    "dist = Categorical(logits=logits)\n",
    "\n",
    "print(dist.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2900,  0.1700,  0.0757, -0.0335], grad_fn=<ViewBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([4]), covariance_matrix: torch.Size([4, 4]))\n"
     ]
    }
   ],
   "source": [
    "mean = actor_ANN(observation)\n",
    "dist = MultivariateNormal(mean, cov_mat)\n",
    "\n",
    "print(mean)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 4., 4., 4.], grad_fn=<MinBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([4]), covariance_matrix: torch.Size([4, 4]))\n"
     ]
    }
   ],
   "source": [
    "mean_st = actor_SNN(obs_st)[0]\n",
    "mean = decode_first_spike(mean_st)\n",
    "dist = MultivariateNormal(mean, cov_mat)\n",
    "print(mean)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2900,  0.1700,  0.0757, -0.0335], grad_fn=<ViewBackward0>)\n",
      "Categorical(logits: torch.Size([4]))\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "logits = actor_ANN(observation)\n",
    "dist = Categorical(logits=logits)\n",
    "\n",
    "print(logits)\n",
    "print(dist)\n",
    "\n",
    "print(dist.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 4., 4., 4.], grad_fn=<MinBackward0>)\n",
      "Categorical(logits: torch.Size([4]))\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "logits_st = actor_SNN(obs_st)[0]\n",
    "logits = decode_first_spike(logits_st)\n",
    "dist = Categorical(logits=logits)\n",
    "\n",
    "print(logits)\n",
    "print(dist)\n",
    "\n",
    "print(dist.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(observation, num_steps, threshold, shift):\n",
    "    \"\"\"\n",
    "    Generate spike trains from a single observation using a fixed global threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - observation: A tensor representing the observation ([observation_dim]).\n",
    "    - num_steps: The number of timesteps for the spike train.\n",
    "    - threshold: A single global threshold value to be used for normalization.\n",
    "    \n",
    "    Returns:\n",
    "    - spike_trains: Tensor of spike trains.\n",
    "    \"\"\"\n",
    "    \n",
    "    shift = shift.numpy()\n",
    "\n",
    "    # Normalize and clip observation\n",
    "    shifted_obs = np.add(observation, shift) \n",
    "\n",
    "    # torch version\n",
    "    #shifted_obs = observation + shift\n",
    "\n",
    "\n",
    "    normalized_obs = shifted_obs / (threshold + 1e-6)  # Avoid division by zero\n",
    "\n",
    "    normalized_obs /= 2\n",
    "    \n",
    "    normalized_obs = normalized_obs.clamp(0, 1)  # Clip values to be within [0, 1]\n",
    "\n",
    "    \n",
    "    # Generate spike trains\n",
    "    spike_trains = spikegen.rate(normalized_obs, num_steps=num_steps)\n",
    "    \n",
    "    # torch version\n",
    "    #return spike_trains\n",
    "\n",
    "    return spike_trains.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 100, 8)\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([6, 100, 4])\n",
      "huh tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 4., 4., 4.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [4., 4., 5., 4.],\n",
      "        [4., 4., 4., 4.]], grad_fn=<MinBackward0>)\n",
      "huhhhhh True\n",
      "tensor([[96., 83., 91., 87.],\n",
      "        [94., 77., 90., 83.],\n",
      "        [91., 77., 87., 80.],\n",
      "        [94., 78., 84., 81.],\n",
      "        [92., 79., 84., 81.],\n",
      "        [90., 80., 87., 84.]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "np_obs = np.array([[1.2, 0.5, 2.0, -5, 1.0, -4.5, 0.8, 0.3],\n",
    "                  [1.2, 0.5, 2.0, -5, 1.0, -4.5, 0.8, 0.3],\n",
    "                  [1.2, 0.5, 2.0, -5, 1.0, -4.5, 0.8, 0.3],\n",
    "                  [1.2, 0.5, 2.0, -5, 1.0, -4.5, 0.8, 0.3],\n",
    "                  [1.2, 0.5, 2.0, -5, 1.0, -4.5, 0.8, 0.3],\n",
    "                  [1.2, 0.5, 2.0, -5, 1.0, -4.5, 0.8, 0.3]])\n",
    "\n",
    "\n",
    "\n",
    "np_obs_st = generate_spike_trains_batched(np_obs, 100, threshold, shift)\n",
    "\n",
    "#print(np_obs_st.requires_grad)\n",
    "\n",
    "\n",
    "print(np_obs_st.shape)\n",
    "\n",
    "yeet = actor_SNN(np_obs_st)[0]\n",
    "\n",
    "print(type(yeet))\n",
    "print(yeet.shape)\n",
    "\n",
    "bruh = decode_first_spike_batched(yeet)\n",
    "\n",
    "print(\"huh\", bruh)\n",
    "print(\"huhhhhh\", bruh.requires_grad)\n",
    "\n",
    "\n",
    "print(get_spike_counts_batched(yeet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
