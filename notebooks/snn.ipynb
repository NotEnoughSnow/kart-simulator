{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.distributions import MultivariateNormal\n",
    "import snntorch.functional as SF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNN def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300  # number of hidden neurons\n",
    "\n",
    "\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_steps):\n",
    "        super(SNN, self).__init__()\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        beta1 = 0.9\n",
    "        beta2 = torch.rand((output_size), dtype = torch.float) # independent decay rate for each leaky neuron in layer 2: [0, 1)\n",
    "\n",
    "        # layername.weight.data += 0.5\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, dtype=torch.float)\n",
    "        self.fc1.weight.data += 0.01\n",
    "        self.lif1 = snn.Leaky(beta=beta1)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size, dtype=torch.float)\n",
    "        self.fc2.weight.data += 0.01\n",
    "        self.lif2 = snn.Leaky(beta=beta2, learn_beta=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        spk2_rec = [] # record output spikes\n",
    "        mem2_rec = [] # record output hidden states\n",
    "\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2) # record spikes\n",
    "            mem2_rec.append(mem2) # record membrane\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=1), torch.stack(mem2_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "\n",
    "class SNN_2(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_steps):\n",
    "        super(SNN_2, self).__init__()\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lif1 = snn.Leaky(beta=0.9)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.lif2 = snn.Leaky(beta=0.9)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.lif3 = snn.Leaky(beta=0.9, learn_beta=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize the membrane potentials\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk3_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x[:,step,:])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 5\n",
    "act_dim = 5\n",
    "\n",
    "# Define the number of time steps for the simulation\n",
    "num_steps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_batch = 4800  # Number of timesteps to run per batch\n",
    "max_timesteps_per_episode = 1600  # Max number of timesteps per episode\n",
    "n_updates_per_iteration = 5  # Number of times to update actor/critic per iteration\n",
    "lr = 0.005  # Learning rate of actor optimizer\n",
    "gamma = 0.95  # Discount factor to be applied when calculating Rewards-To-Go\n",
    "clip = 0.2  # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
    "\n",
    "# Miscellaneous parameters\n",
    "render = True  # If we should render during rollout\n",
    "render_every_i = 10  # Only render every n iterations\n",
    "save_freq = 10  # How often we save in number of iterations\n",
    "seed = None  # Sets the seed of our program, used for reproducibility of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "#### actor and critic networks\n",
    "#### optimizers\n",
    "#### covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Initialize actor and critic networks\n",
    "actor = SNN(obs_dim, act_dim, num_steps).to(device)  # ALG STEP 1\n",
    "critic = SNN(obs_dim, 1, num_steps).to(device)\n",
    "\n",
    "# Initialize optimizers for actor and critic\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "cov_var = torch.full(size=(act_dim,), fill_value=0.5)\n",
    "cov_mat = torch.diag(cov_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "#### rewards to go calculation\n",
    "#### evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_first_spike_batched(spike_trains):\n",
    "    \"\"\"\n",
    "    Decodes the first spike time from spike trains for batched data using 'time to first spike' method.\n",
    "\n",
    "    Parameters:\n",
    "        spike_trains - The batched spike trains with shape (batch_size, num_steps, num_neurons).\n",
    "\n",
    "    Returns:\n",
    "        decoded_vector - The decoded first spike times with shape (batch_size, num_neurons).\n",
    "    \"\"\"\n",
    "    batch_size = spike_trains.size(0)\n",
    "    num_neurons = spike_trains.size(2)\n",
    "    decoded_vectors = []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        decoded_vector = [spike_trains.size(1)+1] * num_neurons\n",
    "        \n",
    "        for neuron_idx in range(num_neurons):\n",
    "            first_spike = (spike_trains[batch_idx, :, neuron_idx] == 1).nonzero(as_tuple=True)[0]\n",
    "            if first_spike.nelement() != 0:\n",
    "                decoded_vector[neuron_idx] = first_spike[0].item() + 1\n",
    "        \n",
    "        decoded_vectors.append(decoded_vector)\n",
    "\n",
    "    return torch.FloatTensor(decoded_vectors)\n",
    "\n",
    "def decode_from_spikes_count(spikes):\n",
    "    spike_counts = torch.sum(spikes, dim=1)\n",
    "    action = torch.zeros(spikes.size(0))\n",
    "    max_spike_count = torch.max(spike_counts)\n",
    "    candidates = torch.where(spike_counts == max_spike_count)[0]\n",
    "    if len(candidates) > 1:\n",
    "        action[torch.multinomial(candidates.float(), 1)] = 1\n",
    "    else:\n",
    "        action[candidates] = 1\n",
    "    return action\n",
    "\n",
    "def encode_to_spikes(data, num_steps):\n",
    "    \"\"\"\n",
    "    Encodes analog signals into spike trains using rate encoding.\n",
    "\n",
    "    Parameters:\n",
    "        data - The continuous-valued data to be encoded.\n",
    "        num_steps - The number of time steps for the spike train.\n",
    "\n",
    "    Returns:\n",
    "        spike_train - The encoded spike train.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-6\n",
    "\n",
    "\n",
    "    # Normalize the data to be between 0 and 1\n",
    "    normalized_data = (data - data.min()) / (data.max() - data.min() + epsilon)\n",
    "\n",
    "    normalized_data = torch.clamp(normalized_data, 0, 1)\n",
    "\n",
    "    # Convert normalized data to spike trains\n",
    "    # TODO rate vs latency vs delta\n",
    "    spike_train = spikegen.rate(normalized_data, num_steps=num_steps)\n",
    "\n",
    "    return spike_train\n",
    "\n",
    "def encode_to_spikes_batched(data, num_steps):\n",
    "    \"\"\"\n",
    "    Encodes analog signals into spike trains using rate encoding.\n",
    "\n",
    "    Parameters:\n",
    "        data - The continuous-valued data to be encoded.\n",
    "        num_steps - The number of time steps for the spike train.\n",
    "\n",
    "    Returns:\n",
    "        spike_train - The encoded spike train.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    min = data.min(dim=1, keepdim=True)[0]\n",
    "    max = data.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "    normalized_data = (data - min) / (max - min + epsilon)\n",
    "    normalized_data = torch.clamp(normalized_data, 0, 1)\n",
    "\n",
    "    spike_train = spikegen.rate(normalized_data, num_steps=num_steps)\n",
    "\n",
    "    return spike_train.transpose(0,1)\n",
    "\n",
    "\n",
    "\n",
    "def compute_rtgs(batch_rews):\n",
    "    \"\"\"\n",
    "        Compute the Reward-To-Go of each timestep in a batch given the rewards.\n",
    "\n",
    "        Parameters:\n",
    "            batch_rews - the rewards in a batch, Shape: (number of episodes, number of timesteps per episode)\n",
    "\n",
    "        Return:\n",
    "            batch_rtgs - the rewards to go, Shape: (number of timesteps in batch)\n",
    "    \"\"\"\n",
    "    # The rewards-to-go (rtg) per episode per batch to return.\n",
    "    # The shape will be (num timesteps per episode)\n",
    "    batch_rtgs = []\n",
    "\n",
    "    # Iterate through each episode\n",
    "    for ep_rews in reversed(batch_rews):\n",
    "\n",
    "        discounted_reward = 0  # The discounted reward so far\n",
    "\n",
    "        # Iterate through all rewards in the episode. We go backwards for smoother calculation of each\n",
    "        # discounted return (think about why it would be harder starting from the beginning)\n",
    "        for rew in reversed(ep_rews):\n",
    "            discounted_reward = rew + discounted_reward * gamma\n",
    "            batch_rtgs.insert(0, discounted_reward)\n",
    "\n",
    "    # Convert the rewards-to-go into a tensor\n",
    "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "\n",
    "    return batch_rtgs\n",
    "\n",
    "def evaluate(batch_obs_ts, batch_acts):\n",
    "    \"\"\"\n",
    "        Estimate the values of each observation, and the log probs of\n",
    "        each action in the most recent batch with the most recent\n",
    "        iteration of the actor network. Should be called from learn.\n",
    "\n",
    "        Parameters:\n",
    "            batch_obs - the observations from the most recently collected batch as a tensor.\n",
    "                        Shape: (number of timesteps in batch, dimension of observation)\n",
    "            batch_acts - the actions from the most recently collected batch as a tensor.\n",
    "                        Shape: (number of timesteps in batch, dimension of action)\n",
    "\n",
    "        Return:\n",
    "            V - the predicted values of batch_obs\n",
    "            log_probs - the log probabilities of the actions taken in batch_acts given batch_obs\n",
    "    \"\"\"\n",
    "\n",
    "    # Query critic network for a value V for each batch_obs. Shape of V should be same as batch_rtgs\n",
    "    V_ts = critic(batch_obs_ts)[0]\n",
    "\n",
    "    V = decode_first_spike_batched(V_ts).squeeze().requires_grad_(True)\n",
    "\n",
    "    # Calculate the log probabilities of batch actions using most recent actor network.\n",
    "    # This segment of code is similar to that in get_action()\n",
    "    mean_ts = actor(batch_obs_ts)[0].detach()\n",
    "\n",
    "    mean = decode_first_spike_batched(mean_ts)\n",
    "\n",
    "    dist = MultivariateNormal(mean, cov_mat)\n",
    "    log_probs = dist.log_prob(batch_acts).requires_grad_(True)\n",
    "\n",
    "    # Return the value vector V of each observation in the batch\n",
    "    # and log probabilities log_probs of each action in the batch\n",
    "    return V, log_probs\n",
    "\n",
    "def get_action(obs):\n",
    "    \"\"\"\n",
    "        Queries an action from the actor network, should be called from rollout.\n",
    "\n",
    "        Parameters:\n",
    "            obs - the observation at the current timestep\n",
    "\n",
    "        Return:\n",
    "            action - the action to take, as a numpy array\n",
    "            log_prob - the log probability of the selected action in the distribution\n",
    "    \"\"\"\n",
    "\n",
    "    obs = torch.tensor(obs)\n",
    "\n",
    "    obs_st = encode_to_spikes_batched(obs.unsqueeze(0), num_steps=num_steps)\n",
    "\n",
    "    # Query the actor network for a mean action\n",
    "    mean_st = actor(obs_st)[0].detach()\n",
    "\n",
    "    mean = decode_first_spike_batched(mean_st)\n",
    "\n",
    "\n",
    "    # Create a distribution with the mean action and std from the covariance matrix above.\n",
    "    # For more information on how this distribution works, check out Andrew Ng's lecture on it:\n",
    "    # https://www.youtube.com/watch?v=JjB58InuTqM\n",
    "    dist = MultivariateNormal(mean, cov_mat)\n",
    "\n",
    "    # Sample an action from the distribution\n",
    "    action = dist.sample()\n",
    "\n",
    "    # Calculate the log probability for that action\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    # Return the sampled action and the log probability of that action in our distribution\n",
    "    return action.detach().numpy(), log_prob.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset, First observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([300.0000, 450.0000,   0.0000,   4.7124,   0.0000])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# first observation, from reset\n",
    "obs = torch.tensor([300.0000, 450.0000,   0.0000,   4.7124,   0.0000])\n",
    "\n",
    "print(obs)\n",
    "\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[300.0000, 450.0000,   0.0000,   4.7124,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "obs = obs.unsqueeze(0)\n",
    "\n",
    "print(obs)\n",
    "\n",
    "\n",
    "obs_st = encode_to_spikes_batched(obs ,num_steps=num_steps)\n",
    "\n",
    "# print(obs_st)\n",
    "\n",
    "#print(obs_st.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get action and log prob of first observation\n",
    "This is a one iteration of get_action() method\n",
    "\n",
    "- Get network output to be used as a mean for the distribution.\n",
    "\n",
    "- Create a distribution with the mean action and std from the covariance matrix above. <br/>\n",
    "For more information on how this distribution works, check out Andrew Ng's lecture on it: <br/>\n",
    "https://www.youtube.com/watch?v=JjB58InuTqM <br/>\n",
    "\n",
    "- Sample an action from the distribution\n",
    "\n",
    "- Calculate the log probability for that action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "tensor([[0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mean_spike_trains = actor(obs_st)\n",
    "\n",
    "mean_spike_trains = mean_spike_trains[0].detach()\n",
    "print(mean_spike_trains)\n",
    "\n",
    "\n",
    "mean = decode_first_spike_batched(mean_spike_trains)\n",
    "mean = (num_steps + 1) - mean\n",
    "\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of distribution  tensor([[0., 0., 0., 0., 0.]])\n",
      "action to take  [[-0.02905154  0.7660263   0.5499304   0.5236275   0.03144384]]\n",
      "log probability of the action  tensor([-4.0271])\n"
     ]
    }
   ],
   "source": [
    "dist = MultivariateNormal(mean, cov_mat)\n",
    "\n",
    "action = dist.sample()\n",
    "\n",
    "log_prob = dist.log_prob(action)\n",
    "\n",
    "print(\"mean of distribution \", mean.detach())\n",
    "print(\"action to take \", action.detach().numpy())\n",
    "print(\"log probability of the action \",log_prob.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 5])\n",
      "[[51.251507 50.864086 51.058483 51.350163 51.643894]]\n",
      "tensor([-3.4842])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_8284\\3488195472.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = torch.tensor(obs)\n"
     ]
    }
   ],
   "source": [
    "batch_obs = torch.tensor([[300.0000, 450.0000,   0.0000,   4.7124,   0.0000],\n",
    "        [299.9333, 450.0000,   3.9801,   4.7124,   0.0000],\n",
    "        [299.8003, 450.0000,   7.9404,   4.7124,   0.0000],\n",
    "        [300.0000, 450.0000,   0.0000,   4.7124,   0.0000],\n",
    "        [299.9333, 450.0000,   3.9801,   4.7124,   0.0000],\n",
    "        [299.8003, 450.0000,   7.9404,   4.7124,   0.0000]])\n",
    "\n",
    "action, log_prob = get_action(batch_obs[0])\n",
    "\n",
    "print(action)\n",
    "print(log_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout\n",
    "\n",
    "- batch observations collected from simulation, first obs O_0 is from reset [0, n-1]\n",
    "- batch actions collected from querying the network given observations [1, n]\n",
    "- batch log probabilities collected from querying the network given observations [1, n]\n",
    "- batch rewards collected from simulation after taking an action. [1, n]\n",
    "- batch lenghts stores batch and episode lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_8284\\3488195472.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = torch.tensor(obs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 5])\n",
      "torch.Size([1, 50, 5])\n",
      "torch.Size([1, 50, 5])\n",
      "torch.Size([1, 50, 5])\n",
      "torch.Size([1, 50, 5])\n",
      "torch.Size([1, 50, 5])\n",
      "batch acts  tensor([[51.8653, 50.4827, 50.7976, 51.7156, 50.3940],\n",
      "        [51.5114, 51.0334, 51.5198, 50.7115, 50.1022],\n",
      "        [50.8452, 50.5212, 51.2185, 51.5430, 51.2571],\n",
      "        [50.4020, 50.0194, 51.5051, 51.1050, 50.8650],\n",
      "        [50.3344, 50.7350, 51.2924, 51.7496, 50.4155],\n",
      "        [50.1316, 51.3448, 50.5148, 51.2064, 51.5824]])\n",
      "batch log probs  tensor([-4.7985, -4.2839, -3.5237, -4.4654, -4.3640, -4.3521])\n",
      "batch rtg  tensor([-3.2398, -2.2304, -1.1539, -3.2398, -2.2304, -1.1539])\n"
     ]
    }
   ],
   "source": [
    "# rollout\n",
    "\n",
    "# batch obs\n",
    "batch_obs = torch.tensor([[300.0000, 450.0000,   0.0000,   4.7124,   0.0000],\n",
    "        [299.9333, 450.0000,   3.9801,   4.7124,   0.0000],\n",
    "        [299.8003, 450.0000,   7.9404,   4.7124,   0.0000],\n",
    "        [300.0000, 450.0000,   0.0000,   4.7124,   0.0000],\n",
    "        [299.9333, 450.0000,   3.9801,   4.7124,   0.0000],\n",
    "        [299.8003, 450.0000,   7.9404,   4.7124,   0.0000]])\n",
    "\n",
    "batch_acts = []\n",
    "batch_log_probs = []\n",
    "\n",
    "for i in range(len(batch_obs)):\n",
    "        # get action takes train spikes\n",
    "        # check if this could be done without a loop\n",
    "        action, log_prob = get_action(batch_obs[i])\n",
    "        batch_acts.append(action)\n",
    "        batch_log_probs.append(log_prob)\n",
    "\n",
    "\n",
    "batch_rews = [[-1.1209449646284, -1.134160306418572, -1.1539176437616128], [-1.1209449646284, -1.134160306418572, -1.1539176437616128]]\n",
    "\n",
    "batch_rtgs = compute_rtgs(batch_rews)\n",
    "\n",
    "batch_lens = [3, 3]\n",
    "\n",
    "batch_acts = torch.tensor(np.array(batch_acts), dtype=torch.float).squeeze()\n",
    "batch_log_probs = torch.tensor(np.array(batch_log_probs), dtype=torch.float).squeeze()\n",
    "\n",
    "print(\"batch acts \",batch_acts)\n",
    "print(\"batch log probs \",batch_log_probs)\n",
    "print(\"batch rtg \",batch_rtgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 50, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_obs_ts = encode_to_spikes_batched(batch_obs, num_steps=num_steps)\n",
    "\n",
    "print(batch_obs_ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate one iteration (for demonstration)\n",
    "- Query critic network for a value V for each batch_obs. Shape of V should be same as batch_rtgs\n",
    "- Calculate the log probabilities of batch actions using most recent actor network.\n",
    "This segment of code is similar to that in get_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5])\n",
      "tensor([51., 51., 51., 51., 51., 51.])\n",
      "tensor([ -6.1439,  -7.2954,  -6.0614,  -6.0476, -16.3022,  -4.9679])\n"
     ]
    }
   ],
   "source": [
    "V_ts = critic(batch_obs_ts)[0].detach()\n",
    "V = decode_first_spike_batched(V_ts).squeeze()\n",
    "\n",
    "\n",
    "mean_ts = actor(batch_obs_ts)[0].detach()\n",
    "\n",
    "mean = decode_first_spike_batched(mean_ts)\n",
    "\n",
    "dist = MultivariateNormal(mean, cov_mat)\n",
    "log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "print(mean.shape)\n",
    "print(V)\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([51., 51., 51., 51., 51., 51.], requires_grad=True)\n",
      "tensor([-54.2398, -53.2304, -52.1539, -54.2398, -53.2304, -52.1539])\n"
     ]
    }
   ],
   "source": [
    "# Calculate advantage at k-th iteration\n",
    "V, _ = evaluate(batch_obs_ts, batch_acts)\n",
    "A_k = batch_rtgs - V.detach()\n",
    "\n",
    "print(V)\n",
    "print(A_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1059, -0.0240,  1.1298, -1.1059, -0.0240,  1.1298])\n"
     ]
    }
   ],
   "source": [
    "# normalizing the advantage\n",
    "A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "print(A_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the network through a number of iterations\n",
    "\n",
    "- Calculate V_phi and pi_theta(a_t | s_t)\n",
    "- Calculate the ratio pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t) <br/>\n",
    "NOTE: we just subtract the logs, which is the same as<br/>\n",
    "dividing the values and then canceling the log with e^log.<br/>\n",
    "For why we use log probabilities instead of actual probabilities,<br/>\n",
    "here's a great explanation:<br/>\n",
    "https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms<br/>\n",
    "TL;DR makes gradient ascent easier behind the scenes.<br/>\n",
    "- Calculate surrogate losses.\n",
    "- Calculate actor and critic losses. <br/>\n",
    "NOTE: we take the negative min of the surrogate losses because we're trying to maximize <br/>\n",
    "the performance function, but Adam minimizes the loss. So minimizing the negative <br/>\n",
    "performance function maximizes it. <br/>\n",
    "- Calculate gradients and perform backward propagation for actor and critic network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## update start ##\n",
      "* V \n",
      " tensor([51., 51., 51., 51., 51., 51.])\n",
      "* curr_log_probs \n",
      " tensor([ -6.1439,  -7.2954,  -6.0614,  -6.0476, -16.3022,  -4.9679])\n",
      "* ratios \n",
      " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "* surr1 \n",
      " tensor([-1.1059, -0.0240,  1.1298, -1.1059, -0.0240,  1.1298])\n",
      "* surr2 \n",
      " tensor([-1.1059, -0.0240,  1.1298, -1.1059, -0.0240,  1.1298])\n",
      "tensor(1.4106e-06, grad_fn=<MeanBackward0>)\n",
      "tensor(2831.8203, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_updates_per_iteration = 1\n",
    "\n",
    "actor_loss_arr = []\n",
    "critic_loss_arr = []\n",
    "\n",
    "\n",
    "for _ in range(num_updates_per_iteration):\n",
    "\n",
    "    print(\"## update start ##\")\n",
    "\n",
    "    V, curr_log_probs = evaluate(batch_obs_ts, batch_acts)\n",
    "\n",
    "\n",
    "    ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "    surr1 = ratios * A_k\n",
    "    surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * A_k\n",
    "\n",
    "    actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "    critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "\n",
    "    actor_loss_arr.append(actor_loss)\n",
    "    critic_loss_arr.append(critic_loss)\n",
    "\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward(retain_graph=True)\n",
    "    actor_optim.step()\n",
    "\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n",
    "\n",
    "\n",
    "    print(\"* V \\n\",V.detach())\n",
    "    print(\"* curr_log_probs \\n\",curr_log_probs.detach())\n",
    "    print(\"* ratios \\n\",ratios.detach())\n",
    "    print(\"* surr1 \\n\",surr1.detach())\n",
    "    print(\"* surr2 \\n\",surr2.detach())\n",
    "\n",
    "print(actor_loss)\n",
    "print(critic_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
