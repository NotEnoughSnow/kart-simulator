{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "import torch, torch.nn as nn\n",
    "from snntorch import surrogate\n",
    "from torch.distributions import MultivariateNormal, Categorical\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64  # Number of hidden neurons\n",
    "\n",
    "\n",
    "class SNN_lrl(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_steps):\n",
    "        super(SNN_lrl, self).__init__()\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        beta1 = 0.9\n",
    "        beta2 = torch.rand((output_size), dtype=torch.float)  # Independent decay rate for each output neuron\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, dtype=torch.float)\n",
    "        self.fc1.weight.data += 0.05\n",
    "        self.lif1 = snn.Leaky(beta=beta1, spike_grad=surrogate.fast_sigmoid())\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size, dtype=torch.float)\n",
    "        self.fc2.weight.data += 0.05\n",
    "        self.lif2 = snn.Leaky(beta=beta2, learn_beta=True, spike_grad=surrogate.fast_sigmoid())\n",
    "\n",
    "        # Linear readout layer\n",
    "        self.readout = nn.Linear(output_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        # Determine if input is batched or not\n",
    "        is_batched = x.dim() == 3  # [batch_size, num_steps, input_size] is 3D\n",
    "\n",
    "        if not is_batched:\n",
    "            # If not batched, add a batch dimension\n",
    "            x = x.unsqueeze(0)  # Shape becomes [1, num_steps, input_size]\n",
    "\n",
    "\n",
    "        batch_size = x.size(0)  # This is 1 if not batched, otherwise the actual batch size\n",
    "\n",
    "        # Initialize membrane potentials\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the spikes from the last layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        # Shape: [batch_size, num_steps, output_size]\n",
    "        spk2_stacked = torch.stack(spk2_rec, dim=1)\n",
    "        mem2_stacked = torch.stack(mem2_rec, dim=1)\n",
    "\n",
    "        # Take the average membrane potential across time (summarize spikes)\n",
    "        # Shape: [batch_size, output_size]\n",
    "        avg_spk2 = torch.mean(spk2_stacked, dim=1)\n",
    "        avg_mem2 = torch.mean(mem2_stacked, dim=1)\n",
    "\n",
    "        # Apply the linear readout layer to the average membrane potential\n",
    "        # Shape: [batch_size, output_size]\n",
    "        readout_output_spk = self.readout(avg_spk2)\n",
    "        readout_output_mem = self.readout(avg_mem2)\n",
    "\n",
    "        if not is_batched:\n",
    "            # Remove the batch dimension if it was added\n",
    "            readout_output_spk = readout_output_spk.squeeze(0)  # Shape becomes [output_size]\n",
    "            readout_output_mem = readout_output_mem.squeeze(0)  # Shape becomes [output_size]\n",
    "\n",
    "            spk2_stacked = spk2_stacked.squeeze(0)  # Shape becomes [num_steps, output_size]\n",
    "\n",
    "\n",
    "        return readout_output_mem, spk2_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64  # Number of hidden neurons\n",
    "\n",
    "class SNN_small(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_steps):\n",
    "        super(SNN_small, self).__init__()\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        beta1 = 0.9\n",
    "        #beta2 = torch.rand((output_size), dtype=torch.float)  # Independent decay rate for each output neuron\n",
    "        beta2 = 0.9\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, dtype=torch.float)\n",
    "        self.fc1.weight.data += 0.01\n",
    "        self.lif1 = snn.Leaky(beta=beta1, spike_grad=surrogate.fast_sigmoid())\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size, dtype=torch.float)\n",
    "        self.fc2.weight.data += 0.01\n",
    "        #self.lif2 = snn.Leaky(beta=beta2, learn_beta=True, spike_grad=surrogate.fast_sigmoid())\n",
    "        self.lif2 = snn.Leaky(beta=beta2, spike_grad=surrogate.fast_sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        # Determine if input is batched or not\n",
    "        is_batched = x.dim() == 3  # [batch_size, num_steps, input_size] is 3D\n",
    "\n",
    "        if not is_batched:\n",
    "            # If not batched, add a batch dimension\n",
    "            x = x.unsqueeze(0)  # Shape becomes [1, num_steps, input_size]\n",
    "\n",
    "\n",
    "        batch_size = x.size(0)  # This is 1 if not batched, otherwise the actual batch size\n",
    "\n",
    "        # Initialize membrane potentials\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the spikes from the last layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "\n",
    "        output_spk = torch.stack(spk2_rec, dim=1)  # Shape: [batch_size, num_steps, output_size]\n",
    "        output_mem = torch.stack(mem2_rec, dim=1)  # Shape: [batch_size, num_steps, output_size]\n",
    "\n",
    "        if not is_batched:\n",
    "            # Remove the batch dimension if it was added\n",
    "            output_spk = output_spk.squeeze(0)  # Shape becomes [num_steps, output_size]\n",
    "            output_mem = output_mem.squeeze(0)  # Shape becomes [num_steps, output_size]\n",
    "\n",
    "\n",
    "        return output_spk, output_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spike_trains(observation, num_steps, threshold, shift):\n",
    "    \"\"\"\n",
    "    Generate spike trains from a single observation using a fixed global threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - observation: A tensor representing the observation ([observation_dim]).\n",
    "    - num_steps: The number of timesteps for the spike train.\n",
    "    - threshold: A single global threshold value to be used for normalization.\n",
    "\n",
    "    Returns:\n",
    "    - spike_trains: Tensor of spike trains.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize and clip observation\n",
    "    shifted_obs = np.add(observation, shift)\n",
    "\n",
    "    # torch version\n",
    "    # shifted_obs = observation + shift\n",
    "\n",
    "    normalized_obs = shifted_obs / (threshold + 1e-6)  # Avoid division by zero\n",
    "\n",
    "    normalized_obs /= 2\n",
    "\n",
    "    normalized_obs = normalized_obs.clamp(0, 1)  # Clip values to be within [0, 1]\n",
    "\n",
    "    # Generate spike trains\n",
    "    spike_trains = spikegen.rate(normalized_obs, num_steps=num_steps)\n",
    "\n",
    "    # torch version\n",
    "    # return spike_trains\n",
    "\n",
    "    return spike_trains.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_first_spike(spike_trains):\n",
    "    \"\"\"\n",
    "    Decodes the first spike time from spike trains using the 'time to first spike' method.\n",
    "\n",
    "    Parameters:\n",
    "        spike_trains - The spike trains with shape (num_steps, num_neurons).\n",
    "\n",
    "    Returns:\n",
    "        decoded_vector - A tensor representing the first spike times for each neuron with gradients retained.\n",
    "    \"\"\"\n",
    "    num_steps, num_neurons = spike_trains.shape\n",
    "\n",
    "    # Create a tensor with time steps and retain gradients\n",
    "    time_tensor = torch.arange(1, num_steps + 1, dtype=torch.float32, requires_grad=True).unsqueeze(1).expand(num_steps,\n",
    "                                                                                                              num_neurons)\n",
    "\n",
    "    # Multiply spike_trains by the time tensor, masking out non-spike entries\n",
    "    spike_times = spike_trains * time_tensor\n",
    "\n",
    "    # Set all zero entries (no spike) to a very high value (greater than num_steps)\n",
    "    spike_times = spike_times + (1 - spike_trains) * (num_steps + 1)\n",
    "\n",
    "    # Find the minimum value in each column (i.e., first spike)\n",
    "    first_spike_times, _ = spike_times.min(dim=0)\n",
    "\n",
    "    # Transform the spike times into a format better suited for a categorical\n",
    "    first_spike_times = (-2 / (num_steps + 1)) * first_spike_times + 2\n",
    "\n",
    "    # Ensure that this tensor retains gradients\n",
    "    return first_spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_spike(spike_trains):\n",
    "    \"\"\"\n",
    "    Decodes the first spike time from spike trains using the 'time to first spike' method.\n",
    "\n",
    "    Parameters:\n",
    "        spike_trains - The spike trains with shape (num_steps, num_neurons).\n",
    "\n",
    "    Returns:\n",
    "        decoded_vector - A tensor representing the first spike times for each neuron with gradients retained.\n",
    "    \"\"\"\n",
    "    num_steps, num_neurons = spike_trains.shape\n",
    "\n",
    "    time_tensor = torch.arange(1, num_steps + 1, dtype=torch.float32, requires_grad=True).unsqueeze(1).expand(num_steps,\n",
    "                                                                                                              num_neurons)\n",
    "    spike_times = spike_trains * time_tensor\n",
    "    spike_times = spike_times + (1 - spike_trains) * (num_steps + 1)\n",
    "    first_spike_times, _ = spike_times.min(dim=0)\n",
    "\n",
    "    return first_spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_first_spike_batched(spike_trains):\n",
    "    \"\"\"\n",
    "    Decodes the first spike time from batched spike trains using the 'time to first spike' method.\n",
    "\n",
    "    Parameters:\n",
    "        spike_trains - The batched spike trains with shape (batch_size, num_steps, num_neurons).\n",
    "\n",
    "    Returns:\n",
    "        decoded_vector - A tensor representing the first spike times for each neuron in each batch with gradients retained.\n",
    "    \"\"\"\n",
    "    batch_size, num_steps, num_neurons = spike_trains.shape\n",
    "\n",
    "    # Create a tensor with time steps and retain gradients\n",
    "    time_tensor = torch.arange(1, num_steps + 1, dtype=torch.float32, requires_grad=True).unsqueeze(0).unsqueeze(\n",
    "        2).expand(batch_size, num_steps, num_neurons)\n",
    "\n",
    "    # Multiply spike_trains by the time tensor, masking out non-spike entries\n",
    "    spike_times = spike_trains * time_tensor\n",
    "\n",
    "    # Set all zero entries (no spike) to a very high value (greater than num_steps)\n",
    "    spike_times = spike_times + (1 - spike_trains) * (num_steps + 1)\n",
    "\n",
    "    # Find the minimum value in each column (i.e., first spike) for each batch\n",
    "    first_spike_times, _ = spike_times.min(dim=1)\n",
    "\n",
    "    # Transform the spike times into a format better suited for a categorical\n",
    "    first_spike_times = (-2 / (num_steps + 1)) * first_spike_times + 2\n",
    "\n",
    "    # Ensure that this tensor retains gradients\n",
    "    return first_spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spike_trains_batched(observations, num_steps, threshold, shift):\n",
    "    \"\"\"\n",
    "    Generate spike trains from batched observations using a fixed global threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - observations: A tensor representing the batched observations ([batch_size, observation_dim]).\n",
    "    - num_steps: The number of timesteps for the spike train.\n",
    "    - threshold: A single global threshold value to be used for normalization.\n",
    "    - shift: A value to shift the observation range to handle negative values.\n",
    "\n",
    "    Returns:\n",
    "    - spike_trains: Tensor of spike trains with shape (batch_size, num_steps, observation_dim).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Normalize and shift observations\n",
    "    normalized_obs = np.add(observations, shift) / (2 * (threshold + 1e-6))  # Avoid division by zero\n",
    "    normalized_obs = normalized_obs.clamp(0, 1)  # Clip values to [0, 1]\n",
    "\n",
    "    # Generate spike trains for each observation in the batch\n",
    "    spike_trains = spikegen.rate(normalized_obs, num_steps=num_steps)\n",
    "\n",
    "    # Rearrange the output to have shape (batch_size, num_steps, observation_dim)\n",
    "    spike_trains = spike_trains.permute(1, 0, 2)\n",
    "\n",
    "    # torch version\n",
    "    # return spike_trains\n",
    "\n",
    "    return spike_trains.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spike_metrics(spk_output):\n",
    "    \"\"\"\n",
    "    Compute the average spike time and ratio of neurons that spike at least once.\n",
    "\n",
    "    Handles both batched ([batch_size, num_steps, output_size]) and unbatched ([num_steps, output_size]) outputs.\n",
    "\n",
    "    Parameters:\n",
    "        spk_output: Spiking activity output from the actor network.\n",
    "                    Shape can be either [batch_size, num_steps, output_size] or [num_steps, output_size].\n",
    "\n",
    "    Returns:\n",
    "        avg_spike_time: The average time at which spikes occur\n",
    "        spike_ratio: The ratio of neurons that spike at least once\n",
    "    \"\"\"\n",
    "    if spk_output.dim() == 3:\n",
    "        # Batched case: [batch_size, num_steps, output_size]\n",
    "        spike_times = get_first_spike_batched(spk_output)\n",
    "        avg_spike_time = torch.mean(spike_times)  # Average spike time\n",
    "\n",
    "        # Calculate the ratio of neurons that spiked at least once per batch\n",
    "        spike_ratio = (spk_output.sum(dim=1) > 0).float().mean()\n",
    "\n",
    "    elif spk_output.dim() == 2:\n",
    "        # Unbatched case: [num_steps, output_size]\n",
    "        spike_times = get_first_spike(spk_output)\n",
    "        avg_spike_time = torch.mean(spike_times)  # Average spike time\n",
    "\n",
    "        # Calculate the ratio of neurons that spiked at least once\n",
    "        spike_ratio = (spk_output.sum(dim=0) > 0).float().mean()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"spk_output must have 2 or 3 dimensions, got shape: {}\".format(spk_output.shape))\n",
    "\n",
    "    return avg_spike_time.detach(), spike_ratio.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 8\n",
    "act_dim = 4\n",
    "num_steps = 32        \n",
    "\n",
    "actor = SNN_lrl(obs_dim, act_dim, num_steps)\n",
    "critic = SNN_lrl(obs_dim, 1, num_steps)\n",
    "\n",
    "threshold = torch.tensor([1.5, 1.5, 5, 5, 3.14, 5, 1, 1])\n",
    "shift = np.array([1.5, 1.5, 5, 5, 3.14, 5, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_batch = np.array([[ 0.11220751, 1.069325, 0.41773742, -0.7382385, -0.02831368, -0.08749236, 0., 0.],\n",
    "[ 0.11624928, 1.0521305, 0.40627155, -0.7642402, -0.03038585, -0.04144341, 0., 0.],\n",
    "[ 0.12014542, 1.0351937, 0.39247927, -0.75280756, -0.03321088, -0.05650074, 0., 0.],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action  tensor([ 1.1359,  1.0008, -0.4039, -0.1630], grad_fn=<SqueezeBackward1>)\n",
      "action  tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 1., 0.]], grad_fn=<SqueezeBackward1>)\n",
      "avg spike time  tensor(10.7500)\n",
      "spike ratio  tensor(0.7500)\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "obs_st = generate_spike_trains(obs_batch[0],\n",
    "                                            num_steps=num_steps,\n",
    "                                            threshold=threshold,\n",
    "                                            shift=shift)\n",
    "\n",
    "obs_st_batch = generate_spike_trains_batched(obs_batch,\n",
    "                                            num_steps=num_steps,\n",
    "                                            threshold=threshold,\n",
    "                                            shift=shift)\n",
    "\n",
    "spk_output, spikes = actor(obs_st)\n",
    "print(\"action \", spk_output)\n",
    "print(\"action \", spikes)\n",
    "\n",
    "avg_spike_time, spike_ratio = compute_spike_metrics(spikes)\n",
    "\n",
    "print(\"avg spike time \", avg_spike_time)\n",
    "print(\"spike ratio \", spike_ratio)\n",
    "print(\"---------------\")\n",
    "\n",
    "logits = spk_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#logits = decode_first_spike(spk_output)\n",
    "\n",
    "dist = Categorical(logits=logits)\n",
    "\n",
    "# Sample an action from the distribution\n",
    "action = dist.sample()\n",
    "\n",
    "# Calculate the log probability for that action\n",
    "log_prob = dist.log_prob(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bb, zz = critic(obs_st_batch)\n",
    "\n",
    "print(bb.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
