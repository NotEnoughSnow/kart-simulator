{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import snntorch\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNN def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300  # number of hidden neurons\n",
    "\n",
    "\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_steps):\n",
    "        super(SNN, self).__init__()\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        beta1 = 0.9 # global decay rate for all leaky neurons in layer 1\n",
    "        beta2 = torch.rand((output_size), dtype = torch.float) # independent decay rate for each leaky neuron in layer 2: [0, 1)\n",
    "\n",
    "        # Initialize layers using snnTorch's Leaky integrate-and-fire neurons\n",
    "        self.fc1 = snn.Linear(input_size, hidden_size)\n",
    "        self.lif1 = snn.Leaky(beta=beta1)  # beta is the decay rate of the membrane potential\n",
    "        self.fc2 = snn.Linear(hidden_size, output_size)\n",
    "        self.lif2 = snn.Leaky(beta=beta2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize the membrane potentials to zero for each forward pass\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        spk2_rec = [] # record output spikes\n",
    "        mem2_rec = [] # record output hidden states\n",
    "\n",
    "        # Loop over time steps\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2) # record spikes\n",
    "            mem2_rec.append(mem2) # record membrane\n",
    "\n",
    "        return torch.stack(spk2_rec), torch.stack(mem2_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_time_to_first_spike(spike_trains):\n",
    "    \n",
    "    decoded_values = torch.full((spike_trains.size(0),), -1.0)\n",
    "\n",
    "    for i, train in enumerate(spike_trains):\n",
    "\n",
    "        first_spike_time = torch.argmax(train) \n",
    "\n",
    "        if train[first_spike_time] == 1:\n",
    "            decoded_values[i] = first_spike_time.float() \n",
    "\n",
    "    return decoded_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_counting_decoding(spikes):\n",
    "    spike_counts = torch.sum(spikes, dim=1)\n",
    "    action = torch.zeros(spikes.size(0))\n",
    "    max_spike_count = torch.max(spike_counts)\n",
    "    candidates = torch.where(spike_counts == max_spike_count)[0]\n",
    "    if len(candidates) > 1:\n",
    "        action[torch.multinomial(candidates.float(), 1)] = 1\n",
    "    else:\n",
    "        action[candidates] = 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import spikegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_to_spikes(data, num_steps):\n",
    "    \"\"\"\n",
    "    Encodes analog signals into spike trains using rate encoding.\n",
    "\n",
    "    Parameters:\n",
    "        data - The continuous-valued data to be encoded.\n",
    "        num_steps - The number of time steps for the spike train.\n",
    "\n",
    "    Returns:\n",
    "        spike_train - The encoded spike train.\n",
    "    \"\"\"\n",
    "    # Normalize the data to be between 0 and 1\n",
    "    normalized_data = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "    # Convert normalized data to spike trains\n",
    "    # TODO rate vs latency vs delta\n",
    "    spike_train = spikegen.rate(normalized_data, num_steps=num_steps)\n",
    "\n",
    "    return spike_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
